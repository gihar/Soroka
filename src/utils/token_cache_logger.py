"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ prompt caching –≤ LLM –∑–∞–ø—Ä–æ—Å–∞—Ö
"""

from typing import Dict, Any, Optional
from loguru import logger


# –°—Ç–æ–∏–º–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (USD –∑–∞ 1K —Ç–æ–∫–µ–Ω–æ–≤)
TOKEN_COSTS = {
    # OpenAI
    "gpt-4o": {"input": 0.0025, "output": 0.010, "cached_input": 0.00125},  # 50% —Å–∫–∏–¥–∫–∞
    "gpt-4o-mini": {"input": 0.00015, "output": 0.0006, "cached_input": 0.000075},  # 50% —Å–∫–∏–¥–∫–∞
    "gpt-4-turbo": {"input": 0.010, "output": 0.030, "cached_input": 0.005},  # 50% —Å–∫–∏–¥–∫–∞
    "gpt-4": {"input": 0.030, "output": 0.060, "cached_input": 0.015},  # 50% —Å–∫–∏–¥–∫–∞
    "gpt-3.5-turbo": {"input": 0.0005, "output": 0.0015, "cached_input": 0.0005},  # –ù–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ
    
    # Anthropic
    "claude-3-5-sonnet-20241022": {"input": 0.003, "output": 0.015, "cached_input": 0.0003},  # 90% —Å–∫–∏–¥–∫–∞
    "claude-3-5-sonnet-20240620": {"input": 0.003, "output": 0.015, "cached_input": 0.0003},  # 90% —Å–∫–∏–¥–∫–∞
    "claude-3-opus-20240229": {"input": 0.015, "output": 0.075, "cached_input": 0.0015},  # 90% —Å–∫–∏–¥–∫–∞
    "claude-3-sonnet-20240229": {"input": 0.003, "output": 0.015, "cached_input": 0.0003},  # 90% —Å–∫–∏–¥–∫–∞
}


def check_cache_support(model_name: str, provider: str) -> Dict[str, Any]:
    """
    –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å prompt caching
    
    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "gpt-4o", "claude-3-5-sonnet")
        provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä API ("openai" –∏–ª–∏ "anthropic")
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–æ–¥–¥–µ—Ä–∂–∫–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è:
        {
            "supported": bool,
            "method": str,  # "automatic" –∏–ª–∏ "explicit"
            "min_tokens": int,
            "discount": float,  # –ü—Ä–æ—Ü–µ–Ω—Ç —Å–∫–∏–¥–∫–∏ (0.5 = 50%, 0.9 = 90%)
            "notes": str
        }
    """
    model_lower = model_name.lower()
    
    if provider == "openai":
        # OpenAI –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        if any(m in model_lower for m in ["gpt-4o", "gpt-4-turbo"]):
            return {
                "supported": True,
                "method": "automatic",
                "min_tokens": 1024,
                "discount": 0.5,  # 50% —Å–∫–∏–¥–∫–∞
                "notes": "OpenAI –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–µ—à–∏—Ä—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã –ø—Ä–æ–º–ø—Ç–æ–≤ >= 1024 —Ç–æ–∫–µ–Ω–æ–≤"
            }
        elif "gpt-4" in model_lower and "vision" not in model_lower:
            return {
                "supported": True,
                "method": "automatic",
                "min_tokens": 1024,
                "discount": 0.5,  # 50% —Å–∫–∏–¥–∫–∞
                "notes": "OpenAI –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫–µ—à–∏—Ä—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã –ø—Ä–æ–º–ø—Ç–æ–≤ >= 1024 —Ç–æ–∫–µ–Ω–æ–≤"
            }
        else:
            return {
                "supported": False,
                "method": None,
                "min_tokens": 0,
                "discount": 0.0,
                "notes": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç prompt caching"
            }
    
    elif provider == "anthropic":
        # Anthropic –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —è–≤–Ω–æ–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è Claude 3 –º–æ–¥–µ–ª–µ–π
        if "claude-3" in model_lower or "claude-3.5" in model_lower:
            return {
                "supported": True,
                "method": "explicit",
                "min_tokens": 1024,
                "discount": 0.9,  # 90% —Å–∫–∏–¥–∫–∞
                "notes": "Anthropic —Ç—Ä–µ–±—É–µ—Ç —è–≤–Ω—ã—Ö cache_control –º–∞—Ä–∫–µ—Ä–æ–≤. –ú–∏–Ω–∏–º—É–º 1024 —Ç–æ–∫–µ–Ω–∞ –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è."
            }
        else:
            return {
                "supported": False,
                "method": None,
                "min_tokens": 0,
                "discount": 0.0,
                "notes": f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç prompt caching"
            }
    
    else:
        return {
            "supported": False,
            "method": None,
            "min_tokens": 0,
            "discount": 0.0,
            "notes": f"–ü—Ä–æ–≤–∞–π–¥–µ—Ä {provider} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç prompt caching"
        }


def _get_token_cost(model_name: str, token_type: str) -> float:
    """
    –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–æ–∏–º–æ—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏
    
    Args:
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        token_type: "input", "output" –∏–ª–∏ "cached_input"
        
    Returns:
        –°—Ç–æ–∏–º–æ—Å—Ç—å –∑–∞ 1K —Ç–æ–∫–µ–Ω–æ–≤ –≤ USD
    """
    model_lower = model_name.lower()
    
    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    if model_lower in TOKEN_COSTS:
        return TOKEN_COSTS[model_lower].get(token_type, 0.0)
    
    # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
    for model_key, costs in TOKEN_COSTS.items():
        if model_key in model_lower:
            return costs.get(token_type, 0.0)
    
    # Fallback - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å gpt-4o –∫–∞–∫ –±–∞–∑–æ–≤—É—é
    return TOKEN_COSTS.get("gpt-4o", {}).get(token_type, 0.0)


def log_cached_tokens_usage(
    response: Any,
    context: str = "",
    model_name: Optional[str] = None,
    provider: str = "openai"
) -> Dict[str, Any]:
    """
    –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ API
    
    Args:
        response: –û—Ç–≤–µ—Ç –æ—Ç OpenAI/Anthropic API —Å usage –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π
        context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –≤—ã–∑–æ–≤–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "Stage 1", "Unified")
        model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–æ–∏–º–æ—Å—Ç–∏
        provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä API ("openai" –∏–ª–∏ "anthropic")
        
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏:
        {
            "prompt_tokens": int,
            "completion_tokens": int,
            "total_tokens": int,
            "cached_tokens": int,
            "cache_hit_rate": float,  # 0.0 - 1.0
            "cost_without_cache": float,
            "cost_with_cache": float,
            "cost_saved": float,
            "savings_percent": float
        }
    """
    metrics = {
        "prompt_tokens": 0,
        "completion_tokens": 0,
        "total_tokens": 0,
        "cached_tokens": 0,
        "cache_hit_rate": 0.0,
        "cost_without_cache": 0.0,
        "cost_with_cache": 0.0,
        "cost_saved": 0.0,
        "savings_percent": 0.0
    }
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º usage –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    if not hasattr(response, 'usage'):
        logger.debug(f"[{context}] Response –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç usage –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
        return metrics
    
    usage = response.usage
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics["prompt_tokens"] = getattr(usage, 'prompt_tokens', 0) or 0
    metrics["completion_tokens"] = getattr(usage, 'completion_tokens', 0) or 0
    metrics["total_tokens"] = getattr(usage, 'total_tokens', 0) or 0
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö
    cached_tokens = 0
    
    # OpenAI: prompt_tokens_details.cached_tokens
    if hasattr(usage, 'prompt_tokens_details') and usage.prompt_tokens_details:
        details = usage.prompt_tokens_details
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø–æ–ª—è
        if hasattr(details, 'cached_tokens'):
            cached_tokens = getattr(details, 'cached_tokens', 0) or 0
        elif hasattr(details, 'cached_prompt_tokens'):
            cached_tokens = getattr(details, 'cached_prompt_tokens', 0) or 0
    
    # Anthropic: cache_creation_input_tokens + cache_read_input_tokens
    if provider == "anthropic":
        cache_read = getattr(usage, 'cache_read_input_tokens', 0) or 0
        cache_creation = getattr(usage, 'cache_creation_input_tokens', 0) or 0
        cached_tokens = cache_read
        
        if cache_creation > 0:
            logger.debug(f"[{context}] –°–æ–∑–¥–∞–Ω–æ –∫–µ—à–∞: {cache_creation} —Ç–æ–∫–µ–Ω–æ–≤")
    
    metrics["cached_tokens"] = cached_tokens
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º cache hit rate
    if metrics["prompt_tokens"] > 0:
        metrics["cache_hit_rate"] = cached_tokens / metrics["prompt_tokens"]
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å
    if model_name:
        input_cost_per_k = _get_token_cost(model_name, "input")
        output_cost_per_k = _get_token_cost(model_name, "output")
        cached_cost_per_k = _get_token_cost(model_name, "cached_input")
        
        # –°—Ç–æ–∏–º–æ—Å—Ç—å –±–µ–∑ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        uncached_prompt_tokens = metrics["prompt_tokens"]
        metrics["cost_without_cache"] = (
            (uncached_prompt_tokens / 1000) * input_cost_per_k +
            (metrics["completion_tokens"] / 1000) * output_cost_per_k
        )
        
        # –°—Ç–æ–∏–º–æ—Å—Ç—å —Å –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        regular_prompt_tokens = metrics["prompt_tokens"] - cached_tokens
        metrics["cost_with_cache"] = (
            (regular_prompt_tokens / 1000) * input_cost_per_k +
            (cached_tokens / 1000) * cached_cost_per_k +
            (metrics["completion_tokens"] / 1000) * output_cost_per_k
        )
        
        # –≠–∫–æ–Ω–æ–º–∏—è
        metrics["cost_saved"] = metrics["cost_without_cache"] - metrics["cost_with_cache"]
        
        if metrics["cost_without_cache"] > 0:
            metrics["savings_percent"] = (metrics["cost_saved"] / metrics["cost_without_cache"]) * 100
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    context_prefix = f"[{context}] " if context else ""
    
    logger.info(
        f"{context_prefix}–¢–æ–∫–µ–Ω—ã: prompt={metrics['prompt_tokens']}, "
        f"completion={metrics['completion_tokens']}, total={metrics['total_tokens']}"
    )
    
    if cached_tokens > 0:
        logger.info(
            f"{context_prefix}üí∞ –ö–µ—à–∏—Ä–æ–≤–∞–Ω–æ: {cached_tokens} —Ç–æ–∫–µ–Ω–æ–≤ "
            f"({metrics['cache_hit_rate']:.1%} –æ—Ç prompt)"
        )
        
        if model_name and metrics["cost_saved"] > 0:
            logger.info(
                f"{context_prefix}üíµ –≠–∫–æ–Ω–æ–º–∏—è: ${metrics['cost_saved']:.4f} "
                f"({metrics['savings_percent']:.1f}%) | "
                f"–°—Ç–æ–∏–º–æ—Å—Ç—å: ${metrics['cost_with_cache']:.4f} –≤–º–µ—Å—Ç–æ ${metrics['cost_without_cache']:.4f}"
            )
    elif metrics["prompt_tokens"] >= 1024:
        # –ö–µ—à–∞ –Ω–µ—Ç, —Ö–æ—Ç—è —Ç–æ–∫–µ–Ω–æ–≤ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ - –≤–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞
        logger.debug(
            f"{context_prefix}‚ö†Ô∏è –ö–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –Ω–µ—Ç, —Ö–æ—Ç—è prompt >= 1024 —Ç–æ–∫–µ–Ω–æ–≤. "
            f"–í–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å —Ç–∞–∫–∏–º –ø—Ä–æ–º–ø—Ç–æ–º –∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç caching."
        )
    
    return metrics


def format_cache_summary(metrics_list: list[Dict[str, Any]]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ–¥–∫—É –ø–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—é –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
    
    Args:
        metrics_list: –°–ø–∏—Å–æ–∫ –º–µ—Ç—Ä–∏–∫ –æ—Ç log_cached_tokens_usage
        
    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ —Å–≤–æ–¥–∫–æ–π
    """
    if not metrics_list:
        return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–∏"
    
    total_prompt = sum(m["prompt_tokens"] for m in metrics_list)
    total_completion = sum(m["completion_tokens"] for m in metrics_list)
    total_cached = sum(m["cached_tokens"] for m in metrics_list)
    total_cost_saved = sum(m["cost_saved"] for m in metrics_list)
    total_cost_with = sum(m["cost_with_cache"] for m in metrics_list)
    total_cost_without = sum(m["cost_without_cache"] for m in metrics_list)
    
    cache_rate = (total_cached / total_prompt * 100) if total_prompt > 0 else 0
    savings_percent = (total_cost_saved / total_cost_without * 100) if total_cost_without > 0 else 0
    
    summary = [
        "üìä –°–≤–æ–¥–∫–∞ –ø–æ –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—é —Ç–æ–∫–µ–Ω–æ–≤:",
        f"   –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(metrics_list)}",
        f"   –¢–æ–∫–µ–Ω—ã: {total_prompt + total_completion:,} (prompt: {total_prompt:,}, completion: {total_completion:,})",
        f"   –ö–µ—à–∏—Ä–æ–≤–∞–Ω–æ: {total_cached:,} —Ç–æ–∫–µ–Ω–æ–≤ ({cache_rate:.1f}% –æ—Ç prompt)",
    ]
    
    if total_cost_without > 0:
        summary.extend([
            f"   –°—Ç–æ–∏–º–æ—Å—Ç—å: ${total_cost_with:.4f} (–±–µ–∑ –∫–µ—à–∞: ${total_cost_without:.4f})",
            f"   üí∞ –≠–∫–æ–Ω–æ–º–∏—è: ${total_cost_saved:.4f} ({savings_percent:.1f}%)"
        ])
    
    return "\n".join(summary)

