"""
ÐœÐ¾Ð´ÑƒÐ»ÑŒ Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ð¸ Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ LLM Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°Ð¼Ð¸
"""

import json
import asyncio
import httpx
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Tuple, List, TYPE_CHECKING
from loguru import logger
from config import settings

import openai
from anthropic import Anthropic

# Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚ Ð´Ð»Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð½Ð¾-Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ñ‹Ñ… Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð²
from src.services.meeting_classifier import meeting_classifier
from src.prompts.specialized_prompts import (
    get_specialized_system_prompt, 
    get_specialized_extraction_instructions
)

# Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚ Ð´Ð»Ñ retry Ð»Ð¾Ð³Ð¸ÐºÐ¸
from src.reliability.retry import RetryManager, LLM_RETRY_CONFIG

if TYPE_CHECKING:
    from src.services.segmentation_service import TranscriptionSegment


class LLMProvider(ABC):
    """ÐÐ±ÑÑ‚Ñ€Ð°ÐºÑ‚Ð½Ñ‹Ð¹ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ LLM Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð²"""
    
    @abstractmethod
    async def generate_protocol(self, transcription: str, template_variables: Dict[str, str], diarization_data: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ Ð¸ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð°"""
        pass
    
    @abstractmethod
    def is_available(self) -> bool:
        """ÐŸÑ€Ð¾Ð²ÐµÑ€Ð¸Ñ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°"""
        pass


# -------------------------------------------------------------
# Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð±Ð¸Ð»Ð´ÐµÑ€Ñ‹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ð¾Ð² Ð´Ð»Ñ Ð²ÑÐµÑ… Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð²
# -------------------------------------------------------------
def _build_system_prompt(
    transcription: Optional[str] = None,
    diarization_analysis: Optional[Dict[str, Any]] = None
) -> str:
    """
    Ð¡Ñ‚Ñ€Ð¾Ð³Ð°Ñ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ð°Ñ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ° Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð°.
    ÐÐ²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÑ‚ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð° ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ.
    
    Args:
        transcription: Ð¢ÐµÐºÑÑ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ (Ð´Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸)
        diarization_analysis: ÐÐ½Ð°Ð»Ð¸Ð· Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ (Ð´Ð»Ñ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸)
        
    Returns:
        Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð¸Ð»Ð¸ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹)
    """
    # Ð•ÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð° ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ñ Ð¸ ÐµÑÑ‚ÑŒ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ
    if settings.meeting_type_detection and transcription:
        try:
            # ÐšÐ»Ð°ÑÑÐ¸Ñ„Ð¸Ñ†Ð¸Ñ€ÑƒÐµÐ¼ Ð²ÑÑ‚Ñ€ÐµÑ‡Ñƒ
            meeting_type, _ = meeting_classifier.classify(
                transcription, 
                diarization_analysis
            )
            
            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
            specialized_prompt = get_specialized_system_prompt(meeting_type)
            logger.info(f"Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑŽ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ñ‚Ð¸Ð¿Ð° Ð²ÑÑ‚Ñ€ÐµÑ‡Ð¸: {meeting_type}")
            return specialized_prompt
            
        except Exception as e:
            logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð¸: {e}. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚")
    
    # Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ð´Ð»Ñ Ð¾Ð±Ñ‰Ð¸Ñ… Ð²ÑÑ‚Ñ€ÐµÑ‡ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¸ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½Ð½Ð¾Ð¹ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸)
    return (
        "Ð¢Ñ‹ â€” Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð¸ÑÑ‚ Ð²Ñ‹ÑÑˆÐµÐ¹ ÐºÐ²Ð°Ð»Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ Ñ Ð¾Ð¿Ñ‹Ñ‚Ð¾Ð¼ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ "
        "Ð´ÐµÐ»Ð¾Ð²Ñ‹Ñ… Ð²ÑÑ‚Ñ€ÐµÑ‡, ÑÐ¾Ð²ÐµÑ‰Ð°Ð½Ð¸Ð¹ Ð¸ Ð¿ÐµÑ€ÐµÐ³Ð¾Ð²Ð¾Ñ€Ð¾Ð².\n\n"
        
        "Ð¢Ð’ÐžÐ¯ Ð ÐžÐ›Ð¬:\n"
        "- Ð˜Ð·Ð²Ð»ÐµÐºÐ°Ñ‚ÑŒ Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ»ÑŽÑ‡ÐµÐ²ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· ÑÑ‚ÐµÐ½Ð¾Ð³Ñ€Ð°Ð¼Ð¼ Ð²ÑÑ‚Ñ€ÐµÑ‡\n"
        "- Ð¡Ð¾Ð·Ð´Ð°Ð²Ð°Ñ‚ÑŒ Ñ‡ÐµÑ‚ÐºÐ¸Ðµ, Ð»Ð°ÐºÐ¾Ð½Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ñ‹\n"
        "- Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÑ‚ÑŒ Ð¾Ð±ÑŠÐµÐºÑ‚Ð¸Ð²Ð½Ð¾ÑÑ‚ÑŒ Ð¸ Ñ„Ð°ÐºÑ‚Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÑƒÑŽ Ñ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ\n\n"
        
        "ÐŸÐ Ð˜ÐÐ¦Ð˜ÐŸÐ« Ð ÐÐ‘ÐžÐ¢Ð«:\n"
        "1. Ð¢ÐžÐ§ÐÐžÐ¡Ð¢Ð¬: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ„Ð°ÐºÑ‚Ñ‹, ÑÐ²Ð½Ð¾ Ð¿Ñ€Ð¸ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ðµ Ð² ÑÑ‚ÐµÐ½Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ðµ\n"
        "2. ÐÐ•Ð¢ Ð”ÐžÐœÐ«Ð¡Ð›ÐžÐ’: ÐÐµ Ð´Ð¾Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹, Ð½Ðµ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð¸Ñ€ÑƒÐ¹, Ð½Ðµ Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾Ñ‚ ÑÐµÐ±Ñ\n"
        "3. ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢: Ð•ÑÐ»Ð¸ ÑƒÐ¿Ð¾Ð¼Ð¸Ð½Ð°ÐµÑ‚ÑÑ Ñ€Ð¾Ð»ÑŒ/Ð´Ð¾Ð»Ð¶Ð½Ð¾ÑÑ‚ÑŒ/ÑÑ€Ð¾Ðº/ÑÑƒÐ¼Ð¼Ð° â€” ÑƒÐºÐ°Ð¶Ð¸ Ð¸Ñ…; ÐµÑÐ»Ð¸ Ð½ÐµÑ‚ â€” Ð½Ðµ Ð¿Ñ€Ð¸Ð´ÑƒÐ¼Ñ‹Ð²Ð°Ð¹\n"
        "4. ÐšÐ ÐÐ¢ÐšÐžÐ¡Ð¢Ð¬: Ð˜Ð·Ð»Ð°Ð³Ð°Ð¹ ÑÑƒÑ‚ÑŒ Ð±ÐµÐ· Ð²Ð¾Ð´Ñ‹, Ð¸Ð·Ð±ÐµÐ³Ð°Ð¹ Ð¸Ð·Ð±Ñ‹Ñ‚Ð¾Ñ‡Ð½Ñ‹Ñ… Ð´ÐµÑ‚Ð°Ð»ÐµÐ¹\n"
        "5. Ð¢Ð•Ð ÐœÐ˜ÐÐžÐ›ÐžÐ“Ð˜Ð¯: Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐ¹ Ð¿Ñ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹ Ð¸ Ð½Ð°Ð·Ð²Ð°Ð½Ð¸Ñ ÐºÐ°Ðº Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»Ðµ\n"
        "6. Ð¡Ð¢Ð˜Ð›Ð¬: ÐžÑ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾-Ð´ÐµÐ»Ð¾Ð²Ð¾Ð¹ ÑÐ·Ñ‹Ðº Ð±ÐµÐ· Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð½Ñ‹Ñ… Ð¾Ð±Ð¾Ñ€Ð¾Ñ‚Ð¾Ð²\n\n"
        
        "Ð§Ð¢Ðž Ð˜Ð“ÐÐžÐ Ð˜Ð ÐžÐ’ÐÐ¢Ð¬:\n"
        "- ÐœÐµÐ¶Ð´Ð¾Ð¼ÐµÑ‚Ð¸Ñ (Ñ-Ñ, Ð¼-Ð¼, Ð½Ñƒ, Ð²Ð¾Ñ‚)\n"
        "- ÐŸÐ¾Ð²Ñ‚Ð¾Ñ€Ñ‹ Ð¸ Ð·Ð°Ð¿Ð¸Ð½ÐºÐ¸\n"
        "- Ð’Ð²Ð¾Ð´Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹ Ð±ÐµÐ· ÑÐ¼Ñ‹ÑÐ»Ð¾Ð²Ð¾Ð¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸\n"
        "- ÐžÑ‚Ð²Ð»ÐµÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ñ‹ Ð½Ðµ Ð¿Ð¾ Ñ‚ÐµÐ¼Ðµ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð¸\n\n"
        
        "Ð§Ð¢Ðž Ð’Ð«Ð”Ð•Ð›Ð¯Ð¢Ð¬:\n"
        "- ÐšÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¸ Ñ€ÐµÐ·Ð¾Ð»ÑŽÑ†Ð¸Ð¸\n"
        "- ÐŸÐ¾Ñ€ÑƒÑ‡ÐµÐ½Ð¸Ñ Ñ ÑƒÐºÐ°Ð·Ð°Ð½Ð¸ÐµÐ¼ Ð¸ÑÐ¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÐµÐ¹ (ÐµÑÐ»Ð¸ ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÑ‚Ñ‹)\n"
        "- Ð¡Ñ€Ð¾ÐºÐ¸, ÑÑƒÐ¼Ð¼Ñ‹, Ð¿Ð¾ÐºÐ°Ð·Ð°Ñ‚ÐµÐ»Ð¸ (Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÑÐ²Ð½Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ñ‹)\n"
        "- ÐšÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð¸ Ð¸Ñ… Ñ€ÐµÑˆÐµÐ½Ð¸Ñ\n"
        "- Ð¡Ð¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¾Ð³Ð¾Ð²Ð¾Ñ€ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸\n\n"
        
        "Ð¤ÐžÐ ÐœÐÐ¢ Ð’Ð«Ð’ÐžÐ”Ð:\n"
        "Ð¡Ñ‚Ñ€Ð¾Ð³Ð¾ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ JSON-Ð¾Ð±ÑŠÐµÐºÑ‚ Ð±ÐµÐ· Ð¾Ð±Ñ€Ð°Ð¼Ð»ÐµÐ½Ð¸Ñ Ð² markdown, Ð±ÐµÐ· ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ², Ð±ÐµÐ· Ð¿Ð¾ÑÑÐ½ÐµÐ½Ð¸Ð¹. "
        "Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ Ð¸Ð»Ð¸ Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ñ‹ â€” Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ 'ÐÐµ ÑƒÐºÐ°Ð·Ð°Ð½Ð¾'.\n\n"
        
        "ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž â€” Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹:\n"
        "- Ð’Ð¡Ð• Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð² JSON Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ ÐŸÐ ÐžÐ¡Ð¢Ð«ÐœÐ˜ Ð¡Ð¢Ð ÐžÐšÐÐœÐ˜ (string)\n"
        "- ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ {} Ð¸Ð»Ð¸ Ð¼Ð°ÑÑÐ¸Ð²Ñ‹ [] Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹ Ð¿Ð¾Ð»ÐµÐ¹\n"
        "- Ð¡Ð¿Ð¸ÑÐºÐ¸ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐ¹ ÐºÐ°Ðº Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ñ€Ð¾Ñ‡Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ñ Ð¼Ð°Ñ€ÐºÐµÑ€Ð°Ð¼Ð¸ '- ' (Ð´ÐµÑ„Ð¸Ñ + Ð¿Ñ€Ð¾Ð±ÐµÐ»)\n"
        "- Ð”Ð°Ñ‚Ñ‹ Ð¸ Ð²Ñ€ÐµÐ¼Ñ: Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ñ‚ÐµÐºÑÑ‚, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ '20 Ð¾ÐºÑ‚ÑÐ±Ñ€Ñ 2024, 14:30'\n"
        "- Ð£Ñ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¸: Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ðµ Ñ‡ÐµÑ€ÐµÐ· Ñ‚Ð¾Ñ‡ÐºÑƒ Ñ Ð·Ð°Ð¿ÑÑ‚Ð¾Ð¹, Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€ 'Ð˜Ð²Ð°Ð½ ÐŸÐµÑ‚Ñ€Ð¾Ð², Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€; ÐœÐ°Ñ€Ð¸Ñ Ð¡Ð¸Ð´Ð¾Ñ€Ð¾Ð²Ð°'\n"
        "- Ð ÐµÑˆÐµÐ½Ð¸Ñ Ð¸ Ð·Ð°Ð´Ð°Ñ‡Ð¸: Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ñ€Ð¾Ñ‡Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ ÑÐ¾ ÑÐ¿Ð¸ÑÐºÐ¾Ð¼ Ñ‡ÐµÑ€ÐµÐ· \\n, ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿ÑƒÐ½ÐºÑ‚ Ñ '- '\n\n"
        
        "ÐŸÐ Ð˜ÐœÐ•Ð  ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž JSON:\n"
        "{\n"
        '  "date": "20 Ð¾ÐºÑ‚ÑÐ±Ñ€Ñ 2024",\n'
        '  "time": "14:30",\n'
        '  "participants": "ÐžÐºÑÐ°Ð½Ð°, Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº; Ð“Ð°Ð»Ñ, Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³; ÐÐ»ÐµÐºÑÐµÐ¹, Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ‚Ð¾Ñ€",\n'
        '  "decisions": "- Ð ÐµÑˆÐµÐ½Ð¸Ðµ 1\\n- Ð ÐµÑˆÐµÐ½Ð¸Ðµ 2\\n- Ð ÐµÑˆÐµÐ½Ð¸Ðµ 3"\n'
        "}\n\n"
        
        "ÐŸÐ Ð˜ÐœÐ•Ð  ÐÐ•ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž JSON (ÐÐ• Ð”Ð•Ð›ÐÐ™ Ð¢ÐÐš):\n"
        "{\n"
        '  "date": {"day": 20, "month": "Ð¾ÐºÑ‚ÑÐ±Ñ€ÑŒ"},  âŒ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ð¹ Ð¾Ð±ÑŠÐµÐºÑ‚\n'
        '  "participants": ["ÐžÐºÑÐ°Ð½Ð°", "Ð“Ð°Ð»Ñ"],  âŒ Ð¼Ð°ÑÑÐ¸Ð²\n'
        '  "decisions": [{"decision": "Ð ÐµÑˆÐµÐ½Ð¸Ðµ 1"}]  âŒ Ð¼Ð°ÑÑÐ¸Ð² Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð²\n'
        "}"
    )


def _build_user_prompt(
    transcription: str,
    template_variables: Dict[str, str],
    diarization_data: Optional[Dict[str, Any]] = None,
) -> str:
    """Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ Ð¸ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸ÑÐ¼Ð¸ Ðº Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ñƒ."""
    # Ð‘Ð»Ð¾Ðº ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° (Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸)
    if diarization_data and diarization_data.get("formatted_transcript"):
        transcription_text = (
            "Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸ÐµÐ¼ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ñ…:\n"
            f"{diarization_data['formatted_transcript']}\n\n"
            "Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ:\n"
            f"- ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ñ…: {diarization_data.get('total_speakers', 'Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾')}\n"
            f"- Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ñ…: {', '.join(diarization_data.get('speakers', []))}\n\n"
            "Ð˜ÑÑ…Ð¾Ð´Ð½Ð°Ñ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ (Ð´Ð»Ñ ÑÐ¿Ñ€Ð°Ð²ÐºÐ¸):\n"
            f"{transcription}\n"
        )
    else:
        transcription_text = (
            "Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ:\n"
            f"{transcription}\n\n"
            "ÐŸÑ€Ð¸Ð¼ÐµÑ‡Ð°Ð½Ð¸Ðµ: Ð”Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ (Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ñ…) Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ð´Ð»Ñ ÑÑ‚Ð¾Ð¹ Ð·Ð°Ð¿Ð¸ÑÐ¸.\n"
        )

    variables_str = "\n".join([f"- {key}: {desc}" for key, desc in template_variables.items()])

    # ÐžÑÐ½Ð¾Ð²Ð½Ð¾Ð¹ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
    user_prompt = (
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
        "Ð˜Ð¡Ð¥ÐžÐ”ÐÐ«Ð• Ð”ÐÐÐÐ«Ð• Ð”Ð›Ð¯ ÐÐÐÐ›Ð˜Ð—Ð\n"
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
        f"{transcription_text}\n"
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
        "ÐŸÐžÐ›Ð¯ Ð”Ð›Ð¯ Ð˜Ð—Ð’Ð›Ð•Ð§Ð•ÐÐ˜Ð¯\n"
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
        f"{variables_str}\n\n"
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
        "Ð˜ÐÐ¡Ð¢Ð Ð£ÐšÐ¦Ð˜Ð˜ ÐŸÐž Ð˜Ð—Ð’Ð›Ð•Ð§Ð•ÐÐ˜Ð®\n"
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
        
        "ðŸ“‹ Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð Ð’Ð«Ð’ÐžÐ”Ð:\n"
        "- Ð’ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ JSON-Ð¾Ð±ÑŠÐµÐºÑ‚ (Ð±ÐµÐ· ```json, Ð±ÐµÐ· markdown)\n"
        "- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¡Ð¢Ð ÐžÐ“Ðž ÑÑ‚Ð¸ ÐºÐ»ÑŽÑ‡Ð¸ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ° Ð¿Ð¾Ð»ÐµÐ¹\n"
        "- Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐ¹ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÐºÐ»ÑŽÑ‡ÐµÐ¹ ÐºÐ°Ðº Ð² ÑÐ¿Ð¸ÑÐºÐµ Ð²Ñ‹ÑˆÐµ\n"
        "- ÐšÐ°Ð¶Ð´Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ â€” ÑÑ‚Ñ€Ð¾ÐºÐ° (UTF-8), Ð‘Ð•Ð— Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ñ… Ð¾Ð±ÑŠÐµÐºÑ‚Ð¾Ð² Ð¸Ð»Ð¸ Ð¼Ð°ÑÑÐ¸Ð²Ð¾Ð²\n\n"
        
        "ðŸ“ Ð¤ÐžÐ ÐœÐÐ¢Ð˜Ð ÐžÐ’ÐÐÐ˜Ð• Ð¢Ð•ÐšÐ¡Ð¢Ð:\n"
        "- Ð”Ð»Ñ ÑÐ¿Ð¸ÑÐºÐ¾Ð²/Ð¿ÐµÑ€ÐµÑ‡Ð¸ÑÐ»ÐµÐ½Ð¸Ð¹: ÐºÐ°Ð¶Ð´Ñ‹Ð¹ Ð¿ÑƒÐ½ÐºÑ‚ Ñ Ð½Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ñ€Ð¾ÐºÐ¸, Ð½Ð°Ñ‡Ð¸Ð½Ð°Ð¹ Ñ '- ' (Ð´ÐµÑ„Ð¸Ñ + Ð¿Ñ€Ð¾Ð±ÐµÐ»)\n"
        "- ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð½ÑƒÐ¼ÐµÑ€Ð°Ñ†Ð¸ÑŽ (1. 2. 3.), Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ð´ÐµÑ„Ð¸ÑÑ‹\n"
        "- ÐÐ• ÑÑ‚Ð°Ð²ÑŒ Ñ‚Ð¾Ñ‡ÐºÑƒ Ð² ÐºÐ¾Ð½Ñ†Ðµ Ð¿ÑƒÐ½ÐºÑ‚Ð° ÑÐ¿Ð¸ÑÐºÐ°\n"
        "- Ð”Ð»Ñ Ð¸Ð¼ÐµÐ½/ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²: Ñ€Ð°Ð·Ð´ÐµÐ»ÑÐ¹ Ñ‚Ð¾Ñ‡ÐºÐ¾Ð¹ Ñ Ð·Ð°Ð¿ÑÑ‚Ð¾Ð¹ (;)\n"
        "- Ð”Ð»Ñ Ð´Ð°Ñ‚/Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸: ÑÐ¾Ñ…Ñ€Ð°Ð½ÑÐ¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ ÐºÐ°Ðº ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÑ‚ Ð² Ñ‚ÐµÐºÑÑ‚Ðµ\n\n"
        
        "ðŸŽ¯ Ð˜Ð—Ð’Ð›Ð•Ð§Ð•ÐÐ˜Ð• Ð”ÐÐÐÐ«Ð¥:\n"
        "- Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸Ð· Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸\n"
        "- Ð•ÑÐ»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÑ‚Ð° Ñ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð¾Ð¼ (Ñ€Ð¾Ð»ÑŒ, ÑÑ€Ð¾Ðº, ÑÑƒÐ¼Ð¼Ð°) â€” ÑƒÐºÐ°Ð¶Ð¸ Ð¿Ð¾Ð»Ð½Ð¾ÑÑ‚ÑŒÑŽ\n"
        "- Ð•ÑÐ»Ð¸ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚, Ð½ÐµÐ¾Ð´Ð½Ð¾Ð·Ð½Ð°Ñ‡Ð½Ñ‹ Ð¸Ð»Ð¸ Ð½ÐµÑÑÐ½Ñ‹ â€” Ð¿Ð¸ÑˆÐ¸ 'ÐÐµ ÑƒÐºÐ°Ð·Ð°Ð½Ð¾'\n"
        "- Ð£Ð±Ð¸Ñ€Ð°Ð¹ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ñ‹, Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÑÐ¹ Ð¸Ð´ÐµÐ½Ñ‚Ð¸Ñ‡Ð½Ñ‹Ðµ Ð¿ÑƒÐ½ÐºÑ‚Ñ‹\n"
        "- Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐ¹ Ñ…Ñ€Ð¾Ð½Ð¾Ð»Ð¾Ð³Ð¸ÑŽ: Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð¿ÑƒÐ½ÐºÑ‚Ð¾Ð² = Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð² Ñ‚ÐµÐºÑÑ‚Ðµ\n\n"
        
        "ðŸ” ÐžÐ‘Ð ÐÐ‘ÐžÐ¢ÐšÐ Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð˜ Ðž Ð“ÐžÐ’ÐžÐ Ð¯Ð©Ð˜Ð¥:\n"
        "- Ð•ÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð° Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ñ: Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¼ÐµÑ‚ÐºÐ¸ 'Ð¡Ð¿Ð¸ÐºÐµÑ€ 1:', 'Ð¡Ð¿Ð¸ÐºÐµÑ€ 2:' Ð¸ Ñ‚.Ð´.\n"
        "- ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐ¹ Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð° Ð´ÐµÐ¹ÑÑ‚Ð²Ð¸Ñ Ð¿Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ñƒ Ð¸Ñ… Ð²Ñ‹ÑÐºÐ°Ð·Ñ‹Ð²Ð°Ð½Ð¸Ð¹\n"
        "- Ð£ÐºÐ°Ð·Ñ‹Ð²Ð°Ð¹ ÐºÑ‚Ð¾ Ð¿Ñ€Ð¸Ð½ÑÐ» Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ Ð¸Ð»Ð¸ Ð²Ð·ÑÐ» Ð½Ð° ÑÐµÐ±Ñ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÑÑ‚Ð²Ð¾\n"
        "- Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚: 'Ð—Ð°Ð´Ð°Ñ‡Ð° â€” ÐžÑ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹: [Ð¸Ð¼Ñ Ð¸Ð»Ð¸ Ð¡Ð¿Ð¸ÐºÐµÑ€ N]'\n\n"
        
        "ðŸ§¹ Ð§Ð¢Ðž ÐžÐ¢Ð¤Ð˜Ð›Ð¬Ð¢Ð ÐžÐ’Ð«Ð’ÐÐ¢Ð¬:\n"
        "- ÐœÐµÐ¶Ð´Ð¾Ð¼ÐµÑ‚Ð¸Ñ, Ð·Ð°Ð¿Ð¸Ð½ÐºÐ¸, Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ñ‹ ÑÐ»Ð¾Ð²\n"
        "- Ð’Ð²Ð¾Ð´Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð·Ñ‹ Ð±ÐµÐ· ÑÐ¼Ñ‹ÑÐ»Ð¾Ð²Ð¾Ð¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸\n"
        "- Ð Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ñ‹ Ð½Ðµ Ð¿Ð¾ Ñ‚ÐµÐ¼Ðµ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð¸\n"
        "- Ð¢ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸Ð¸ ('Ð½Ðµ ÑÐ»Ñ‹ÑˆÐ½Ð¾', 'Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð¸Ñ‚Ðµ' Ð¸ Ñ‚.Ð´.)\n\n"
        
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
        "ÐŸÐ Ð˜ÐœÐ•Ð Ð« ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž Ð¤ÐžÐ ÐœÐÐ¢Ð˜Ð ÐžÐ’ÐÐÐ˜Ð¯\n"
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
        
        "âœ… ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐž:\n"
        "{\n"
        "  \"participants\": \"Ð˜Ð²Ð°Ð½ Ð˜Ð²Ð°Ð½Ð¾Ð², Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€; ÐœÐ°Ñ€Ð¸Ñ ÐŸÐµÑ‚Ñ€Ð¾Ð²Ð°, Ñ€ÑƒÐºÐ¾Ð²Ð¾Ð´Ð¸Ñ‚ÐµÐ»ÑŒ Ð¾Ñ‚Ð´ÐµÐ»Ð° Ð¿Ñ€Ð¾Ð´Ð°Ð¶; ÐÐ»ÐµÐºÑÐµÐ¹ Ð¡Ð¸Ð´Ð¾Ñ€Ð¾Ð²\",\n"
        "  \"main_topic\": \"ÐŸÐ»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¸Ð½Ð³Ð¾Ð²Ð¾Ð¹ ÐºÐ°Ð¼Ð¿Ð°Ð½Ð¸Ð¸ Ð½Ð° Q2 2024\",\n"
        "  \"decisions\": \"- Ð£Ð²ÐµÐ»Ð¸Ñ‡Ð¸Ñ‚ÑŒ Ð±ÑŽÐ´Ð¶ÐµÑ‚ Ð½Ð° digital-Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¸Ð½Ð³ Ð½Ð° 30%\\n- Ð£Ñ‚Ð²ÐµÑ€Ð´Ð¸Ñ‚ÑŒ Ð½Ð¾Ð²ÑƒÑŽ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸ÑŽ Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð¶ÐµÐ½Ð¸Ñ Ð² ÑÐ¾Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ñ… ÑÐµÑ‚ÑÑ…\\n- ÐžÑ‚Ð»Ð¾Ð¶Ð¸Ñ‚ÑŒ Ð·Ð°Ð¿ÑƒÑÐº Ñ€ÐµÐºÐ»Ð°Ð¼Ñ‹ Ð½Ð° Ð¢Ð’ Ð´Ð¾ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ³Ð¾ ÐºÐ²Ð°Ñ€Ñ‚Ð°Ð»Ð°\",\n"
        "  \"action_items\": \"- ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¿Ñ€ÐµÐ·ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑŽ Ð½Ð¾Ð²Ð¾Ð¹ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸ Ðº 15 Ð¼Ð°Ñ€Ñ‚Ð° â€” ÐžÑ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹: ÐœÐ°Ñ€Ð¸Ñ ÐŸÐµÑ‚Ñ€Ð¾Ð²Ð°\\n- Ð¡Ð¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ñ‚ÑŒ Ð±ÑŽÐ´Ð¶ÐµÑ‚ Ñ Ñ„Ð¸Ð½Ð°Ð½ÑÐ¾Ð²Ñ‹Ð¼ Ð¾Ñ‚Ð´ÐµÐ»Ð¾Ð¼ â€” ÐžÑ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹: Ð˜Ð²Ð°Ð½ Ð˜Ð²Ð°Ð½Ð¾Ð²\\n- ÐŸÑ€Ð¾Ð²ÐµÑÑ‚Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· ÐºÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚Ð¾Ð² â€” ÐžÑ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹: Ð¾Ñ‚Ð´ÐµÐ» Ð¼Ð°Ñ€ÐºÐµÑ‚Ð¸Ð½Ð³Ð°\",\n"
        "  \"deadlines\": \"- ÐŸÑ€ÐµÐ·ÐµÐ½Ñ‚Ð°Ñ†Ð¸Ñ ÑÑ‚Ñ€Ð°Ñ‚ÐµÐ³Ð¸Ð¸: 15 Ð¼Ð°Ñ€Ñ‚Ð° 2024\\n- Ð¡Ð¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð¸Ðµ Ð±ÑŽÐ´Ð¶ÐµÑ‚Ð°: Ð´Ð¾ ÐºÐ¾Ð½Ñ†Ð° Ñ‚ÐµÐºÑƒÑ‰ÐµÐ¹ Ð½ÐµÐ´ÐµÐ»Ð¸\\n- ÐÐ½Ð°Ð»Ð¸Ð· ÐºÐ¾Ð½ÐºÑƒÑ€ÐµÐ½Ñ‚Ð¾Ð²: Ðº ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¼Ñƒ ÑÐ¾Ð²ÐµÑ‰Ð°Ð½Ð¸ÑŽ\",\n"
        "  \"issues\": \"- ÐÐµÐ´Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ñ‹Ð¹ Ð¾Ñ…Ð²Ð°Ñ‚ Ñ†ÐµÐ»ÐµÐ²Ð¾Ð¹ Ð°ÑƒÐ´Ð¸Ñ‚Ð¾Ñ€Ð¸Ð¸ Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¼Ð¸ ÐºÐ°Ð½Ð°Ð»Ð°Ð¼Ð¸\\n- Ð’Ñ‹ÑÐ¾ÐºÐ°Ñ ÑÑ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð¿Ñ€Ð¸Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°\\n- ÐÐµÐ¾Ð±Ñ…Ð¾Ð´Ð¸Ð¼Ð¾ÑÑ‚ÑŒ Ð¾Ð±Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ñ ÐºÑ€ÐµÐ°Ñ‚Ð¸Ð²Ð¾Ð²\"\n"
        "}\n\n"
        
        "âŒ ÐÐ•ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐž:\n"
        "{\n"
        "  \"participants\": [\"Ð˜Ð²Ð°Ð½\", \"ÐœÐ°Ñ€Ð¸Ñ\"],  âŒ Ð¼Ð°ÑÑÐ¸Ð² Ð²Ð¼ÐµÑÑ‚Ð¾ ÑÑ‚Ñ€Ð¾ÐºÐ¸\n"
        "  \"decisions\": \"1) Ð ÐµÑˆÐµÐ½Ð¸Ðµ Ð¾Ð´Ð¸Ð½ 2) Ð ÐµÑˆÐµÐ½Ð¸Ðµ Ð´Ð²Ð°.\",  âŒ Ð½ÑƒÐ¼ÐµÑ€Ð°Ñ†Ð¸Ñ + Ñ‚Ð¾Ñ‡ÐºÐ¸\n"
        "  \"action_items\": \"ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²Ð¸Ñ‚ÑŒ Ð¿Ñ€ÐµÐ·ÐµÐ½Ñ‚Ð°Ñ†Ð¸ÑŽ (ÐœÐ°Ñ€Ð¸Ñ)\",  âŒ Ð±ÐµÐ· Ð´ÐµÑ„Ð¸ÑÐ°, Ð½ÐµÐ¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚\n"
        "  \"deadlines\": \"Ð¡Ñ€Ð¾Ñ‡Ð½Ð¾, ÐºÐ°Ðº Ð¼Ð¾Ð¶Ð½Ð¾ Ð±Ñ‹ÑÑ‚Ñ€ÐµÐµ\",  âŒ Ð½ÐµÑ‚ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð¸ÐºÐ¸, Ñ…Ð¾Ñ‚Ñ Ð¾Ð½Ð° Ð¼Ð¾Ð³Ð»Ð° Ð±Ñ‹Ñ‚ÑŒ\n"
        "  \"extra_field\": \"...\",  âŒ Ð¿Ð¾Ð»Ðµ Ð½Ðµ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ°\n"
        "  \"budget\": \"50000 Ñ€ÑƒÐ±Ð»ÐµÐ¹ (Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾)\"  âŒ Ð´Ð¾Ð¼Ñ‹ÑÐ»Ñ‹ Ð² ÑÐºÐ¾Ð±ÐºÐ°Ñ…\n"
        "}\n\n"
        
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n"
        "Ð¡ÐŸÐ•Ð¦Ð˜Ð¤Ð˜Ð§ÐÐ«Ð• ÐŸÐ ÐÐ’Ð˜Ð›Ð ÐŸÐž Ð¢Ð˜ÐŸÐÐœ ÐŸÐžÐ›Ð•Ð™\n"
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
        
        "ðŸ‘¥ Ð£Ñ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¸ (participants):\n"
        "- Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚: 'Ð˜Ð¼Ñ Ð¤Ð°Ð¼Ð¸Ð»Ð¸Ñ[, Ð´Ð¾Ð»Ð¶Ð½Ð¾ÑÑ‚ÑŒ][; Ð¡Ð»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸Ðº]'\n"
        "- Ð”Ð¾Ð»Ð¶Ð½Ð¾ÑÑ‚ÑŒ ÑƒÐºÐ°Ð·Ñ‹Ð²Ð°Ð¹ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ ÐµÑÐ»Ð¸ ÑÐ²Ð½Ð¾ Ð½Ð°Ð·Ð²Ð°Ð½Ð°\n"
        "- Ð•ÑÐ»Ð¸ Ð¸Ð¼ÐµÐ½Ð° Ð½Ðµ ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÑ‚Ñ‹: 'Ð¡Ð¿Ð¸ÐºÐµÑ€ 1; Ð¡Ð¿Ð¸ÐºÐµÑ€ 2; Ð¡Ð¿Ð¸ÐºÐµÑ€ 3'\n\n"
        
        "ðŸ“Œ Ð ÐµÑˆÐµÐ½Ð¸Ñ (decisions):\n"
        "- Ð¢Ð¾Ð»ÑŒÐºÐ¾ ÑÐ¾Ð³Ð»Ð°ÑÐ¾Ð²Ð°Ð½Ð½Ñ‹Ðµ, ÑƒÑ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ðµ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ\n"
        "- Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÐ° Ð´Ð¾Ð»Ð¶Ð½Ð° Ð¾Ñ‚Ñ€Ð°Ð¶Ð°Ñ‚ÑŒ ÑÑƒÑ‚ÑŒ Ð±ÐµÐ· Ð»Ð¸ÑˆÐ½Ð¸Ñ… ÑÐ»Ð¾Ð²\n"
        "- Ð•ÑÐ»Ð¸ ÐµÑÑ‚ÑŒ ÑƒÑÐ»Ð¾Ð²Ð¸Ñ â€” ÑƒÐºÐ°Ð¶Ð¸ Ð¸Ñ… ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¾\n\n"
        
        "âœ… Ð—Ð°Ð´Ð°Ñ‡Ð¸/Ð¿Ð¾Ñ€ÑƒÑ‡ÐµÐ½Ð¸Ñ (action_items):\n"
        "- Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚: '- ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ â€” ÐžÑ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹: [Ð¸Ð¼Ñ/Ñ€Ð¾Ð»ÑŒ/Ð¡Ð¿Ð¸ÐºÐµÑ€ N]'\n"
        "- Ð•ÑÐ»Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð½Ðµ Ð½Ð°Ð·Ð²Ð°Ð½ ÑÐ²Ð½Ð¾, Ð½Ð¾ Ð¿Ð¾Ð½ÑÑ‚ÐµÐ½ Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° â€” ÑƒÐºÐ°Ð¶Ð¸\n"
        "- Ð•ÑÐ»Ð¸ Ð½ÐµÐ¿Ð¾Ð½ÑÑ‚Ð½Ð¾ ÐºÑ‚Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹: '- ÐžÐ¿Ð¸ÑÐ°Ð½Ð¸Ðµ Ð·Ð°Ð´Ð°Ñ‡Ð¸ â€” ÐžÑ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹: ÐÐµ ÑƒÐºÐ°Ð·Ð°Ð½Ð¾'\n\n"
        
        "â° Ð¡Ñ€Ð¾ÐºÐ¸ (deadlines):\n"
        "- Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚: '- Ð—Ð°Ð´Ð°Ñ‡Ð°/ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ: ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ð¹ ÑÑ€Ð¾Ðº'\n"
        "- Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐ¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ Ð´Ð°Ñ‚ ÐºÐ°Ðº Ð² Ð¾Ñ€Ð¸Ð³Ð¸Ð½Ð°Ð»Ðµ\n"
        "- ÐžÑ‚Ð½Ð¾ÑÐ¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ ÑÑ€Ð¾ÐºÐ¸: 'Ðº ÐºÐ¾Ð½Ñ†Ñƒ Ð½ÐµÐ´ÐµÐ»Ð¸', 'Ðº ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ Ð²ÑÑ‚Ñ€ÐµÑ‡Ðµ'\n\n"
        
        "âš ï¸ ÐŸÑ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹/Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ (issues/questions):\n"
        "- Ð¤Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€ÑƒÐ¹ ÑÑƒÑ‚ÑŒ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ ÐºÑ€Ð°Ñ‚ÐºÐ¾\n"
        "- Ð£Ð±Ð¸Ñ€Ð°Ð¹ ÑÐ¼Ð¾Ñ†Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½ÑƒÑŽ Ð¾ÐºÑ€Ð°ÑÐºÑƒ, Ð¾ÑÑ‚Ð°Ð²Ð»ÑÐ¹ Ñ„Ð°ÐºÑ‚Ñ‹\n"
        "- Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐ¹ ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ñ‹ Ð² Ð¾Ð´Ð¸Ð½ Ð¿ÑƒÐ½ÐºÑ‚\n\n"
        
        "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
        
        "ÐÐÐ§Ð˜ÐÐÐ™ ÐÐÐÐ›Ð˜Ð—. Ð’ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ JSON Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ².\n"
    )
    return user_prompt


class OpenAIProvider(LLMProvider):
    """ÐŸÑ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€ Ð´Ð»Ñ OpenAI GPT"""
    
    def __init__(self):
        self.client = None
        self.http_client = None
        if settings.openai_api_key:
            openai.api_key = settings.openai_api_key
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ HTTP ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸ SSL Ð¸ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ð¾Ð¼ Ð¸Ð· Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº
            import httpx
            self.http_client = httpx.Client(verify=settings.ssl_verify, timeout=settings.llm_timeout_seconds)
            self.client = openai.OpenAI(
                api_key=settings.openai_api_key,
                base_url=settings.openai_base_url,
                http_client=self.http_client
            )
    
    def is_available(self) -> bool:
        return self.client is not None and settings.openai_api_key is not None
    
    async def generate_protocol(self, transcription: str, template_variables: Dict[str, str], diarization_data: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ OpenAI GPT"""
        if not self.is_available():
            raise ValueError("OpenAI API Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½")
        
        # Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ‹
        system_prompt = _build_system_prompt()
        user_prompt = _build_user_prompt(transcription, template_variables, diarization_data)
        
        try:
            # Ð’Ñ‹Ð±Ð¾Ñ€ Ð¿Ñ€ÐµÑÐµÑ‚Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸, ÐµÑÐ»Ð¸ Ð¿ÐµÑ€ÐµÐ´Ð°Ð½ ÐºÐ»ÑŽÑ‡
            selected_model = settings.openai_model
            selected_base_url = settings.openai_base_url or "https://api.openai.com/v1"
            model_key = kwargs.get("openai_model_key")
            if model_key:
                try:
                    preset = next((p for p in settings.openai_models if p.key == model_key), None)
                except Exception:
                    preset = None
                if preset:
                    selected_model = preset.model
                    if getattr(preset, 'base_url', None):
                        selected_base_url = preset.base_url
            
            # ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð½ÑƒÐ¶Ð½Ð¾Ð³Ð¾ base_url (Ð¿Ð¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ self.client)
            client = self.client
            if client is None or (selected_base_url and getattr(client, 'base_url', None) != selected_base_url):
                client = openai.OpenAI(
                    api_key=settings.openai_api_key,
                    base_url=selected_base_url,
                    http_client=self.http_client
                )

            # Ð”Ð¸Ð°Ð³Ð½Ð¾ÑÑ‚Ð¸ÐºÐ° Ð·Ð°Ð¿Ñ€Ð¾ÑÐ° (Ð±ÐµÐ· ÑƒÑ‚ÐµÑ‡ÐºÐ¸ Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸)
            base_url = selected_base_url or "https://api.openai.com/v1"
            sys_msg = "Ð¢Ñ‹ - ÑÑ‚Ñ€Ð¾Ð³Ð¸Ð¹ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð¾Ð² Ð²ÑÑ‚Ñ€ÐµÑ‡..."
            user_len = len(user_prompt)
            transcript_len = len(transcription)
            vars_count = len(template_variables)
            logger.info(
                f"OpenAI Ð·Ð°Ð¿Ñ€Ð¾Ñ: model={selected_model}, base_url={base_url}, "
                f"vars={vars_count}, transcription_chars={transcript_len}, prompt_chars={user_len}"
            )
            _snippet = user_prompt[:400].replace("\n", " ")
            logger.debug(f"OpenAI prompt (Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ 400): {_snippet}...")

            logger.info(f"ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð² OpenAI Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒÑŽ {selected_model}")
            
            # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ extra_headers Ð´Ð»Ñ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ†Ð¸Ð¸
            extra_headers = {}
            if settings.http_referer:
                extra_headers["HTTP-Referer"] = settings.http_referer
            if settings.x_title:
                extra_headers["X-Title"] = settings.x_title
            
            # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ, Ñ‡Ñ‚Ð¾Ð±Ñ‹ Ð½Ðµ Ð±Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ event loop
            async def _call_openai():
                return await asyncio.to_thread(
                    client.chat.completions.create,
                    model=selected_model,
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=0.1,
                    response_format={"type": "json_object"},
                    extra_headers=extra_headers
                )
            response = await _call_openai()
            logger.info("ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ OpenAI API")
            
            content = response.choices[0].message.content
            logger.info(f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ OpenAI (Ð´Ð»Ð¸Ð½Ð°: {len(content) if content else 0}): {content[:200] if content else 'None'}...")
            
            if not content or not content.strip():
                raise ValueError("ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ OpenAI API")
            
            try:
                # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ñ€Ð°ÑÐ¿Ð°Ñ€ÑÐ¸Ñ‚ÑŒ ÐºÐ°Ðº JSON Ð½Ð°Ð¿Ñ€ÑÐ¼ÑƒÑŽ (Ð¾Ð¶Ð¸Ð´Ð°ÐµÑ‚ÑÑ Ð¿Ñ€Ð¸ response_format=json_object)
                return json.loads(content)
            except json.JSONDecodeError as e:
                # ÐœÑÐ³ÐºÐ¸Ð¹ Ð¿Ð°Ñ€ÑÐµÑ€: Ð¿Ñ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð²Ñ‹Ñ€ÐµÐ·Ð°Ñ‚ÑŒ JSON Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð° (ÐºÐ°Ðº Ñƒ Anthropic)
                logger.warning(f"ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON (Ð¿Ñ€ÑÐ¼Ð°Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ°). ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°: {e}")
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                json_str = content[start_idx:end_idx] if start_idx != -1 and end_idx > start_idx else content
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError as e2:
                    logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ OpenAI (Ð¿Ð¾ÑÐ»Ðµ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ): {e2}")
                    logger.error(f"Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {content}")
                    raise ValueError(f"ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ Ð¾Ñ‚ OpenAI: {e2}")
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ OpenAI API: {e}")
            raise


class AnthropicProvider(LLMProvider):
    """ÐŸÑ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€ Ð´Ð»Ñ Anthropic Claude"""
    
    def __init__(self):
        self.client = None
        if settings.anthropic_api_key:
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ HTTP ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ°Ð¼Ð¸ SSL Ð¸ Ñ‚Ð°Ð¹Ð¼Ð°ÑƒÑ‚Ð¾Ð¼ Ð¸Ð· Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº
            import httpx
            http_client = httpx.Client(verify=settings.ssl_verify, timeout=settings.llm_timeout_seconds)
            self.client = Anthropic(
                api_key=settings.anthropic_api_key,
                http_client=http_client
            )
    
    def is_available(self) -> bool:
        return self.client is not None and settings.anthropic_api_key is not None
    
    async def generate_protocol(self, transcription: str, template_variables: Dict[str, str], diarization_data: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Anthropic Claude"""
        if not self.is_available():
            raise ValueError("Anthropic API Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½")
        
        # Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ‹
        system_prompt = _build_system_prompt()
        prompt = _build_user_prompt(transcription, template_variables, diarization_data)
        
        try:
            base_url = "Anthropic SDK"
            user_len = len(prompt)
            transcript_len = len(transcription)
            vars_count = len(template_variables)
            logger.info(
                f"Anthropic Ð·Ð°Ð¿Ñ€Ð¾Ñ: model=claude-3-haiku-20240307, base={base_url}, "
                f"vars={vars_count}, transcription_chars={transcript_len}, prompt_chars={user_len}"
            )
            _a_snippet = prompt[:400].replace("\n", " ")
            logger.debug(f"Anthropic prompt (Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚ 400): {_a_snippet}...")
            
            # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ extra_headers Ð´Ð»Ñ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ†Ð¸Ð¸
            extra_headers = {}
            if settings.http_referer:
                extra_headers["HTTP-Referer"] = settings.http_referer
            if settings.x_title:
                extra_headers["X-Title"] = settings.x_title
            
            # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹ Ð²Ñ‹Ð·Ð¾Ð² ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Anthropic Ð² Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð¼ Ð¿Ð¾Ñ‚Ð¾ÐºÐµ
            async def _call_anthropic():
                return await asyncio.to_thread(
                    self.client.messages.create,
                    model="claude-3-haiku-20240307",
                    max_tokens=2000,
                    temperature=0.1,
                    system=system_prompt,
                    messages=[
                        {"role": "user", "content": prompt}
                    ],
                    extra_headers=extra_headers
                )
            response = await _call_anthropic()
            
            content = response.content[0].text
            logger.info(f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Anthropic (Ð´Ð»Ð¸Ð½Ð°: {len(content) if content else 0}): {content[:200] if content else 'None'}...")
            
            if not content or not content.strip():
                raise ValueError("ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Anthropic API")
            
            # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ JSON Ð¸Ð· Ð¾Ñ‚Ð²ÐµÑ‚Ð°
            start_idx = content.find('{')
            end_idx = content.rfind('}') + 1
            json_str = content[start_idx:end_idx] if start_idx != -1 and end_idx != 0 else content
            
            try:
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ Anthropic: {e}")
                logger.error(f"Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {content}")
                raise ValueError(f"ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ Ð¾Ñ‚ Anthropic: {e}")
            
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Anthropic API: {e}")
            raise


class YandexGPTProvider(LLMProvider):
    """ÐŸÑ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€ Ð´Ð»Ñ Yandex GPT"""
    
    def __init__(self):
        self.api_key = settings.yandex_api_key
        self.folder_id = settings.yandex_folder_id
    
    def is_available(self) -> bool:
        return self.api_key is not None and self.folder_id is not None
    
    async def generate_protocol(self, transcription: str, template_variables: Dict[str, str], diarization_data: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Yandex GPT"""
        if not self.is_available():
            raise ValueError("Yandex GPT API Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½")
        
        # Ð£Ð½Ð¸Ñ„Ð¸Ñ†Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒÑÐºÐ¸Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ‹
        system_prompt = _build_system_prompt()
        prompt = _build_user_prompt(transcription, template_variables, diarization_data)
        
        headers = {
            "Authorization": f"Api-Key {self.api_key}",
            "Content-Type": "application/json"
        }
        
        if settings.http_referer:
            headers["Referer"] = settings.http_referer
        if settings.x_title:
            headers["X-Title"] = settings.x_title
        
        data = {
            "modelUri": f"gpt://{self.folder_id}/yandexgpt-lite",
            "completionOptions": {
                "stream": False,
                "temperature": 0.1,
                "maxTokens": 2000
            },
            "messages": [
                {
                    "role": "system",
                    "text": system_prompt
                },
                {
                    "role": "user", 
                    "text": prompt
                }
            ]
        }
        
        try:
            async with httpx.AsyncClient(verify=settings.ssl_verify) as client:
                response = await client.post(
                    "https://llm.api.cloud.yandex.net/foundationModels/v1/completion",
                    headers=headers,
                    json=data,
                    timeout=settings.llm_timeout_seconds
                )
                response.raise_for_status()
                
                result = response.json()
                content = result["result"]["alternatives"][0]["message"]["text"]
                logger.info(f"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Yandex GPT (Ð´Ð»Ð¸Ð½Ð°: {len(content) if content else 0}): {content[:200] if content else 'None'}...")
                
                if not content or not content.strip():
                    raise ValueError("ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ Yandex GPT API")
                
                # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ JSON Ð¸Ð· Ð¾Ñ‚Ð²ÐµÑ‚Ð°
                start_idx = content.find('{')
                end_idx = content.rfind('}') + 1
                json_str = content[start_idx:end_idx] if start_idx != -1 and end_idx != 0 else content
                
                try:
                    return json.loads(json_str)
                except json.JSONDecodeError as e:
                    logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ Yandex GPT: {e}")
                    logger.error(f"Ð¡Ð¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ð°: {content}")
                    raise ValueError(f"ÐÐµÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹Ð¹ JSON Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ Ð¾Ñ‚ Yandex GPT: {e}")
                
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ñ€Ð°Ð±Ð¾Ñ‚Ðµ Ñ Yandex GPT API: {e}")
            raise


class LLMManager:
    """ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ð¼Ð¸ LLM Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°Ð¼Ð¸"""
    
    def __init__(self):
        self.providers = {
            "openai": OpenAIProvider(),
            "anthropic": AnthropicProvider(),
            "yandex": YandexGPTProvider()
        }
    
    def get_available_providers(self) -> Dict[str, str]:
        """ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ ÑÐ¿Ð¸ÑÐ¾Ðº Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð²"""
        available = {}
        provider_names = {
            "openai": "OpenAI GPT",
            "anthropic": "Anthropic Claude",
            "yandex": "Yandex GPT"
        }
        
        for key, provider in self.providers.items():
            if provider.is_available():
                available[key] = provider_names[key]
        
        return available
    
    async def generate_protocol(self, provider_name: str, transcription: str, 
                              template_variables: Dict[str, str], diarization_data: Optional[Dict[str, Any]] = None, **kwargs) -> Dict[str, Any]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ ÑƒÐºÐ°Ð·Ð°Ð½Ð½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°"""
        if provider_name not in self.providers:
            raise ValueError(f"ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€: {provider_name}")
        
        provider = self.providers[provider_name]
        if not provider.is_available():
            raise ValueError(f"ÐŸÑ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€ {provider_name} Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½")
        
        # ÐŸÐµÑ€ÐµÐ´Ð°ÐµÐ¼ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð°Ñ€Ð³ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ (Ð½Ð°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, openai_model_key)
        return await provider.generate_protocol(transcription, template_variables, diarization_data, **kwargs)
    
    async def generate_protocol_with_fallback(self, preferred_provider: str, transcription: str, 
                                            template_variables: Dict[str, str], diarization_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» Ñ Ð¿ÐµÑ€ÐµÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸ÐµÐ¼ Ð½Ð° Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€ Ð² ÑÐ»ÑƒÑ‡Ð°Ðµ Ð¾ÑˆÐ¸Ð±ÐºÐ¸"""
        available_providers = list(self.get_available_providers().keys())
        
        if not available_providers:
            raise ValueError("ÐÐµÑ‚ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ñ… LLM Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð²")
        
        # Ð¡Ð½Ð°Ñ‡Ð°Ð»Ð° Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€
        providers_to_try = [preferred_provider] if preferred_provider in available_providers else []
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ñ‹ ÐºÐ°Ðº Ñ€ÐµÐ·ÐµÑ€Ð²Ð½Ñ‹Ðµ
        for provider in available_providers:
            if provider not in providers_to_try:
                providers_to_try.append(provider)
        
        last_error = None
        for provider_name in providers_to_try:
            try:
                logger.info(f"ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð° Ñ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð¼: {provider_name}")
                result = await self.generate_protocol(provider_name, transcription, template_variables, diarization_data)
                logger.info(f"Ð£ÑÐ¿ÐµÑˆÐ½Ð¾ ÑÐ³ÐµÐ½ÐµÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» Ñ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð¼: {provider_name}")
                return result
            except Exception as e:
                last_error = e
                logger.warning(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ñ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð¼ {provider_name}: {e}")
                continue
        
        # Ð•ÑÐ»Ð¸ Ð²ÑÐµ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ñ‹ Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸
        raise ValueError(f"Ð’ÑÐµ Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ñ‹ Ð½Ðµ ÑÑ€Ð°Ð±Ð¾Ñ‚Ð°Ð»Ð¸. ÐŸÐ¾ÑÐ»ÐµÐ´Ð½ÑÑ Ð¾ÑˆÐ¸Ð±ÐºÐ°: {last_error}")


# ===================================================================
# Ð”Ð’Ð£Ð¥Ð­Ð¢ÐÐŸÐÐÐ¯ Ð“Ð•ÐÐ•Ð ÐÐ¦Ð˜Ð¯ ÐŸÐ ÐžÐ¢ÐžÐšÐžÐ›Ð
# ===================================================================

def _build_extraction_prompt(
    transcription: str,
    template_variables: Dict[str, str],
    diarization_data: Optional[Dict[str, Any]] = None,
) -> str:
    """
    ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð¿ÐµÑ€Ð²Ð¾Ð³Ð¾ ÑÑ‚Ð°Ð¿Ð°: Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸
    """
    # Ð‘Ð»Ð¾Ðº ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° (Ñ ÑƒÑ‡Ñ‘Ñ‚Ð¾Ð¼ Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸)
    if diarization_data and diarization_data.get("formatted_transcript"):
        transcription_text = (
            "Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ Ñ Ñ€Ð°Ð·Ð´ÐµÐ»ÐµÐ½Ð¸ÐµÐ¼ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ñ…:\n"
            f"{diarization_data['formatted_transcript']}\n\n"
            "Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ:\n"
            f"- ÐšÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ñ…: {diarization_data.get('total_speakers', 'Ð½ÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ð¾')}\n"
            f"- Ð¡Ð¿Ð¸ÑÐ¾Ðº Ð³Ð¾Ð²Ð¾Ñ€ÑÑ‰Ð¸Ñ…: {', '.join(diarization_data.get('speakers', []))}\n\n"
        )
    else:
        transcription_text = f"Ð¢Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ñ:\n{transcription}\n\n"
    
    variables_str = "\n".join([f"- {key}: {desc}" for key, desc in template_variables.items()])
    
    prompt = f"""Ð­Ð¢ÐÐŸ 1: Ð˜Ð—Ð’Ð›Ð•Ð§Ð•ÐÐ˜Ð• Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð˜

{transcription_text}

Ð—ÐÐ”ÐÐ§Ð:
Ð˜Ð·Ð²Ð»ÐµÐºÐ¸ Ð¸Ð· Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… Ð¿Ð¾Ð»ÐµÐ¹:
{variables_str}

Ð¢Ð Ð•Ð‘ÐžÐ’ÐÐÐ˜Ð¯:
1. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ñ„Ð°ÐºÑ‚Ñ‹ Ð¸Ð· Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸
2. Ð•ÑÐ»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð° ÑÐ²Ð½Ð¾ - Ð¿Ð¸ÑˆÐ¸ "ÐÐµ ÑƒÐºÐ°Ð·Ð°Ð½Ð¾"
3. Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐ¹ Ñ…Ñ€Ð¾Ð½Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº
4. ÐÐ• Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð¸Ñ€ÑƒÐ¹ Ð¸ ÐÐ• Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹
5. Ð”Ð»Ñ ÑÐ¿Ð¸ÑÐºÐ¾Ð² Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚: "- Ð¿ÑƒÐ½ÐºÑ‚1\\n- Ð¿ÑƒÐ½ÐºÑ‚2"
6. Ð”Ð»Ñ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð² Ñ Ñ€Ð¾Ð»ÑÐ¼Ð¸: "Ð˜Ð¼Ñ, Ð´Ð¾Ð»Ð¶Ð½Ð¾ÑÑ‚ÑŒ; Ð¡Ð»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ð¹ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸Ðº"

ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž â€” Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ:
- Ð’Ð¡Ð• Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ ÐŸÐ ÐžÐ¡Ð¢Ð«ÐœÐ˜ Ð¡Ð¢Ð ÐžÐšÐÐœÐ˜ (string)
- ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ {{}} Ð¸Ð»Ð¸ Ð¼Ð°ÑÑÐ¸Ð²Ñ‹ [] Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹
- Ð”Ð°Ñ‚Ñ‹: "20 Ð¾ÐºÑ‚ÑÐ±Ñ€Ñ 2024", ÐÐ• {{"day": 20, "month": "Ð¾ÐºÑ‚ÑÐ±Ñ€ÑŒ"}}
- Ð£Ñ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¸: "Ð˜Ð¼Ñ, Ñ€Ð¾Ð»ÑŒ; Ð˜Ð¼Ñ2, Ñ€Ð¾Ð»ÑŒ2", ÐÐ• [{{"name": "Ð˜Ð¼Ñ", "role": "Ñ€Ð¾Ð»ÑŒ"}}]
- Ð¡Ð¿Ð¸ÑÐºÐ¸: "- ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚1\\n- ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚2", ÐÐ• ["ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚1", "ÑÐ»ÐµÐ¼ÐµÐ½Ñ‚2"]

ÐŸÐ Ð˜ÐœÐ•Ð  ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž JSON:
{{
  "date": "20 Ð¾ÐºÑ‚ÑÐ±Ñ€Ñ 2024",
  "participants": "ÐžÐºÑÐ°Ð½Ð°, Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº; Ð“Ð°Ð»Ñ, Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³",
  "decisions": "- Ð ÐµÑˆÐµÐ½Ð¸Ðµ 1\\n- Ð ÐµÑˆÐµÐ½Ð¸Ðµ 2"
}}

Ð¤ÐžÐ ÐœÐÐ¢ Ð’Ð«Ð’ÐžÐ”Ð:
Ð’Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ JSON-Ð¾Ð±ÑŠÐµÐºÑ‚ Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ° Ð¿Ð¾Ð»ÐµÐ¹ Ð²Ñ‹ÑˆÐµ.
ÐšÐ°Ð¶Ð´Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ - ÑÑ‚Ñ€Ð¾ÐºÐ° (UTF-8).

Ð’Ñ‹Ð²ÐµÐ´Ð¸ Ð¢ÐžÐ›Ð¬ÐšÐž JSON, Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ²."""

    return prompt


def _build_reflection_prompt(
    extracted_data: Dict[str, Any],
    transcription: str,
    template_variables: Dict[str, str],
    diarization_analysis: Optional[Dict[str, Any]] = None
) -> str:
    """
    ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð²Ñ‚Ð¾Ñ€Ð¾Ð³Ð¾ ÑÑ‚Ð°Ð¿Ð°: Ð¿Ñ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ
    """
    extracted_json = json.dumps(extracted_data, ensure_ascii=False, indent=2)
    
    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð°Ð½Ð°Ð»Ð¸Ð· Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
    diarization_context = ""
    if diarization_analysis:
        speakers_info = diarization_analysis.get('speakers', {})
        if speakers_info:
            diarization_context = "\n\nÐÐÐÐ›Ð˜Ð— Ð£Ð§ÐÐ¡Ð¢ÐÐ˜ÐšÐžÐ’:\n"
            for speaker_id, info in speakers_info.items():
                role = info.get('role', 'ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸Ðº')
                time_percent = info.get('speaking_time_percent', 0)
                diarization_context += f"- {speaker_id} ({role}): {time_percent:.1f}% Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸\n"
    
    prompt = f"""Ð­Ð¢ÐÐŸ 2: ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð˜ Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐ˜Ð•

Ð˜Ð—Ð’Ð›Ð•Ð§Ð•ÐÐÐ«Ð• Ð”ÐÐÐÐ«Ð• (ÑÑ‚Ð°Ð¿ 1):
{extracted_json}
{diarization_context}

Ð—ÐÐ”ÐÐ§Ð:
ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒ Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð½Ñ‹Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ:

1. ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ ÐŸÐžÐ›ÐÐžÐ¢Ð«:
   - Ð’ÑÐµ Ð»Ð¸ Ð²Ð°Ð¶Ð½Ñ‹Ðµ Ð¼Ð¾Ð¼ÐµÐ½Ñ‚Ñ‹ Ð¸Ð· Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸ Ð¾Ñ‚Ñ€Ð°Ð¶ÐµÐ½Ñ‹?
   - ÐÐµÑ‚ Ð»Ð¸ Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½Ñ‹Ñ… Ñ€ÐµÑˆÐµÐ½Ð¸Ð¹, Ð·Ð°Ð´Ð°Ñ‡ Ð¸Ð»Ð¸ Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼?
   - Ð”Ð¾ÑÑ‚Ð°Ñ‚Ð¾Ñ‡Ð½Ð¾ Ð»Ð¸ Ð´ÐµÑ‚Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ Ð¿Ð¾Ð»Ñ?

2. ÐŸÐ ÐžÐ’Ð•Ð ÐšÐ Ð¢ÐžÐ§ÐÐžÐ¡Ð¢Ð˜:
   - Ð’ÑÐµ Ð»Ð¸ Ñ„Ð°ÐºÑ‚Ñ‹ ÑÐ¾Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÑƒÑŽÑ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸?
   - ÐÐµÑ‚ Ð»Ð¸ Ð´Ð¾Ð¼Ñ‹ÑÐ»Ð¾Ð² Ð¸Ð»Ð¸ Ð¸Ð½Ñ‚ÐµÑ€Ð¿Ñ€ÐµÑ‚Ð°Ñ†Ð¸Ð¹?
   - ÐšÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð½Ñ‹ Ð»Ð¸ Ð¸Ð¼ÐµÐ½Ð° Ð¸ Ñ‚ÐµÑ€Ð¼Ð¸Ð½Ñ‹?

3. Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐÐ˜Ð• Ð”Ð˜ÐÐ Ð˜Ð—ÐÐ¦Ð˜Ð˜:
   - Ð£ÐºÐ°Ð·Ð°Ð½Ñ‹ Ð»Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð·Ð° Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸Ð· Ñ‡Ð¸ÑÐ»Ð° ÑÐ¿Ð¸ÐºÐµÑ€Ð¾Ð²?
   - ÐžÑ‚Ñ€Ð°Ð¶ÐµÐ½ Ð»Ð¸ Ð²ÐºÐ»Ð°Ð´ Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²?
   - Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð° Ð»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ñ€Ð¾Ð»ÑÑ… ÑÐ¿Ð¸ÐºÐµÑ€Ð¾Ð²?

4. Ð¡Ð¢Ð Ð£ÐšÐ¢Ð£Ð Ð:
   - ÐŸÑ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ð¾ Ð»Ð¸ Ð¾Ñ‚Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹ ÑÐ¿Ð¸ÑÐºÐ¸ (Ñ Ð´ÐµÑ„Ð¸ÑÐ°Ð¼Ð¸)?
   - ÐÐµÑ‚ Ð»Ð¸ Ð»Ð¸ÑˆÐ½ÐµÐ¹ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸?
   - Ð›Ð¾Ð³Ð¸Ñ‡ÐµÐ½ Ð»Ð¸ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº Ð¿ÑƒÐ½ÐºÑ‚Ð¾Ð²?

Ð˜ÐÐ¡Ð¢Ð Ð£ÐšÐ¦Ð˜Ð˜ ÐŸÐž Ð£Ð›Ð£Ð§Ð¨Ð•ÐÐ˜Ð®:
- Ð•ÑÐ»Ð¸ Ð½Ð°ÑˆÐµÐ» Ð¿Ñ€Ð¾Ð¿ÑƒÑ‰ÐµÐ½Ð½ÑƒÑŽ Ð²Ð°Ð¶Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ - Ð´Ð¾Ð±Ð°Ð²ÑŒ ÐµÑ‘
- Ð•ÑÐ»Ð¸ Ð½Ð°ÑˆÐµÐ» Ð½ÐµÑ‚Ð¾Ñ‡Ð½Ð¾ÑÑ‚ÑŒ - Ð¸ÑÐ¿Ñ€Ð°Ð²ÑŒ ÐµÑ‘
- Ð•ÑÐ»Ð¸ Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ Ñ„Ð¾Ñ€Ð¼ÑƒÐ»Ð¸Ñ€Ð¾Ð²ÐºÑƒ - ÑƒÐ»ÑƒÑ‡ÑˆÐ¸
- Ð•ÑÐ»Ð¸ Ð¼Ð¾Ð¶Ð½Ð¾ Ð´Ð¾Ð±Ð°Ð²Ð¸Ñ‚ÑŒ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¸Ð· Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ - Ð´Ð¾Ð±Ð°Ð²ÑŒ
- ÐÐ• Ð´Ð¾Ð±Ð°Ð²Ð»ÑÐ¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ, ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ð¹ ÐÐ•Ð¢ Ð² Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸

Ð¤ÐžÐ ÐœÐÐ¢ Ð’Ð«Ð’ÐžÐ”Ð:
Ð’Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ JSON-Ð¾Ð±ÑŠÐµÐºÑ‚ Ñ Ñ‚ÐµÐ¼Ð¸ Ð¶Ðµ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸, Ð½Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÑÐ¼Ð¸.
Ð’Ñ‹Ð²ÐµÐ´Ð¸ Ð¢ÐžÐ›Ð¬ÐšÐž JSON, Ð±ÐµÐ· ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ²."""

    return prompt


async def generate_protocol_two_stage(
    manager: 'LLMManager',
    provider_name: str,
    transcription: str,
    template_variables: Dict[str, str],
    diarization_data: Optional[Dict[str, Any]] = None,
    diarization_analysis: Optional[Dict[str, Any]] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    Ð”Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð°: Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ + Ñ€ÐµÑ„Ð»ÐµÐºÑÐ¸Ñ
    
    Args:
        manager: ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ LLM
        provider_name: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°
        transcription: Ð¢ÐµÐºÑÑ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸
        template_variables: ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð°
        diarization_data: Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        diarization_analysis: ÐÐ½Ð°Ð»Ð¸Ð· Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        **kwargs: Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
        
    Returns:
        Ð£Ð»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»
    """
    logger.info("ÐÐ°Ñ‡Ð°Ð»Ð¾ Ð´Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ð¾Ð¹ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð°")
    
    # Ð­Ð¢ÐÐŸ 1: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸
    logger.info("Ð­Ñ‚Ð°Ð¿ 1: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸")
    extraction_prompt = _build_extraction_prompt(transcription, template_variables, diarization_data)
    
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸ ÐµÑÐ»Ð¸ Ð²ÐºÐ»ÑŽÑ‡ÐµÐ½Ð°)
    system_prompt = _build_system_prompt(transcription, diarization_analysis)
    
    # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ Ð¿ÐµÑ€Ð²Ñ‹Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
    if provider_name == "openai":
        provider = manager.providers[provider_name]
        openai_model_key = kwargs.get("openai_model_key")
        
        # Ð’Ñ‹Ð±Ð¾Ñ€ Ð¿Ñ€ÐµÑÐµÑ‚Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
        selected_model = settings.openai_model
        selected_base_url = settings.openai_base_url or "https://api.openai.com/v1"
        
        if openai_model_key:
            try:
                preset = next((p for p in settings.openai_models if p.key == openai_model_key), None)
                if preset:
                    selected_model = preset.model
                    if getattr(preset, 'base_url', None):
                        selected_base_url = preset.base_url
            except Exception:
                pass
        
        # ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð½ÑƒÐ¶Ð½Ð¾Ð³Ð¾ base_url
        client = provider.client
        if client is None or (selected_base_url and getattr(client, 'base_url', None) != selected_base_url):
            client = openai.OpenAI(
                api_key=settings.openai_api_key,
                base_url=selected_base_url,
                http_client=provider.http_client
            )
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ extra_headers Ð´Ð»Ñ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ†Ð¸Ð¸
        extra_headers = {}
        if settings.http_referer:
            extra_headers["HTTP-Referer"] = settings.http_referer
        if settings.x_title:
            extra_headers["X-Title"] = settings.x_title
        
        # Ð­Ñ‚Ð°Ð¿ 1: Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ
        async def _call_openai_stage1():
            return await asyncio.to_thread(
                client.chat.completions.create,
                model=selected_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": extraction_prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"},
                extra_headers=extra_headers
            )
        
        response1 = await _call_openai_stage1()
        content1 = response1.choices[0].message.content
        
        try:
            extracted_data = json.loads(content1)
        except json.JSONDecodeError as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON Ð½Ð° ÑÑ‚Ð°Ð¿Ðµ 1: {e}")
            # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ JSON Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°
            start_idx = content1.find('{')
            end_idx = content1.rfind('}') + 1
            json_str = content1[start_idx:end_idx] if start_idx != -1 and end_idx > start_idx else content1
            extracted_data = json.loads(json_str)
        
        logger.info(f"Ð­Ñ‚Ð°Ð¿ 1 Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½, Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¾ {len(extracted_data)} Ð¿Ð¾Ð»ÐµÐ¹")
        
        # Ð­Ð¢ÐÐŸ 2: Ð ÐµÑ„Ð»ÐµÐºÑÐ¸Ñ Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ
        logger.info("Ð­Ñ‚Ð°Ð¿ 2: Ð ÐµÑ„Ð»ÐµÐºÑÐ¸Ñ Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ")
        reflection_prompt = _build_reflection_prompt(
            extracted_data, transcription, template_variables, diarization_analysis
        )
        
        async def _call_openai_stage2():
            return await asyncio.to_thread(
                client.chat.completions.create,
                model=selected_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": reflection_prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"},
                extra_headers=extra_headers
            )
        
        response2 = await _call_openai_stage2()
        content2 = response2.choices[0].message.content
        finish_reason = response2.choices[0].finish_reason
        
        # Ð›Ð¾Ð³Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð½Ð¾Ð³Ð¾ Ð¾Ñ‚Ð²ÐµÑ‚Ð°
        logger.info(f"Ð­Ñ‚Ð°Ð¿ 2: Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½ Ð¾Ñ‚Ð²ÐµÑ‚ Ð´Ð»Ð¸Ð½Ð¾Ð¹ {len(content2) if content2 else 0} ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð², finish_reason={finish_reason}")
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° Ð½Ð° Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚
        if not content2 or not content2.strip():
            logger.warning(f"Ð­Ñ‚Ð°Ð¿ 2: Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð¾Ñ‚ API. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÑÑ‚Ð°Ð¿Ð° 1")
            logger.debug(f"Response details: finish_reason={finish_reason}, model={selected_model}")
            return extracted_data
        
        try:
            improved_data = json.loads(content2)
        except json.JSONDecodeError as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON Ð½Ð° ÑÑ‚Ð°Ð¿Ðµ 2: {e}")
            logger.error(f"Content preview (Ð¿ÐµÑ€Ð²Ñ‹Ðµ 500 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²): {content2[:500]}")
            
            # ÐŸÐ¾Ð¿Ñ‹Ñ‚ÐºÐ° Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ JSON Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°
            start_idx = content2.find('{')
            end_idx = content2.rfind('}') + 1
            
            if start_idx != -1 and end_idx > start_idx:
                json_str = content2[start_idx:end_idx]
                try:
                    improved_data = json.loads(json_str)
                    logger.info("JSON ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½ Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°")
                except json.JSONDecodeError as e2:
                    logger.error(f"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð¸Ð·Ð²Ð»ÐµÑ‡ÑŒ JSON: {e2}. Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÑÑ‚Ð°Ð¿Ð° 1")
                    return extracted_data
            else:
                logger.error("JSON Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ. Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚ ÑÑ‚Ð°Ð¿Ð° 1")
                return extracted_data
        
        logger.info(f"Ð­Ñ‚Ð°Ð¿ 2 Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
        return improved_data
    
    else:
        # Ð”Ð»Ñ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð² Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´
        logger.warning(f"Ð”Ð²ÑƒÑ…ÑÑ‚Ð°Ð¿Ð½Ð°Ñ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð´Ð»Ñ {provider_name}, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´")
        return await manager.generate_protocol(
            provider_name, transcription, template_variables, diarization_data, **kwargs
        )


# ===================================================================
# CHAIN-OF-THOUGHT Ð”Ð›Ð¯ Ð”Ð›Ð˜ÐÐÐ«Ð¥ Ð’Ð¡Ð¢Ð Ð•Ð§
# ===================================================================

def _build_segment_analysis_prompt(
    segment_text: str,
    segment_id: int,
    total_segments: int,
    template_variables: Dict[str, str]
) -> str:
    """
    ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð° Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸
    """
    variables_str = "\n".join([f"- {key}: {desc}" for key, desc in template_variables.items()])
    
    prompt = f"""CHAIN-OF-THOUGHT: ÐÐÐÐ›Ð˜Ð— Ð¡Ð•Ð“ÐœÐ•ÐÐ¢Ð {segment_id + 1} Ð˜Ð— {total_segments}

Ð¡Ð•Ð“ÐœÐ•ÐÐ¢ Ð¢Ð ÐÐÐ¡ÐšÐ Ð˜ÐŸÐ¦Ð˜Ð˜:
{segment_text}

Ð—ÐÐ”ÐÐ§Ð:
ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÑ‚Ð¾Ñ‚ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð¸ Ð¸ Ð¸Ð·Ð²Ð»ÐµÐºÐ¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð´Ð»Ñ ÑÐ»ÐµÐ´ÑƒÑŽÑ‰Ð¸Ñ… ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹:
{variables_str}

Ð’ÐÐ–ÐÐž:
- Ð­Ñ‚Ð¾ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚ {segment_id + 1} Ð¸Ð· {total_segments} Ñ‡Ð°ÑÑ‚ÐµÐ¹ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð¸
- Ð˜Ð·Ð²Ð»ÐµÐºÐ°Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ð­Ð¢ÐžÐ“Ðž ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°
- Ð•ÑÐ»Ð¸ Ð² ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ðµ Ð½ÐµÑ‚ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ ÐºÐ°ÐºÐ¾Ð¹-Ñ‚Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¸ - Ð¿Ð¸ÑˆÐ¸ "ÐÐµÑ‚ Ð² ÑÑ‚Ð¾Ð¼ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ðµ"
- Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐ¹ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚: ÑÑ‚Ð¾ Ñ‡Ð°ÑÑ‚ÑŒ Ð±Ð¾Ð»ÐµÐµ Ð´Ð»Ð¸Ð½Ð½Ð¾Ð¹ Ð²ÑÑ‚Ñ€ÐµÑ‡Ð¸
- Ð”Ð»Ñ ÑÐ¿Ð¸ÑÐºÐ¾Ð² Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚: "- Ð¿ÑƒÐ½ÐºÑ‚1\\n- Ð¿ÑƒÐ½ÐºÑ‚2"

Ð¤ÐžÐ ÐœÐÐ¢ Ð’Ð«Ð’ÐžÐ”Ð:
JSON-Ð¾Ð±ÑŠÐµÐºÑ‚ Ñ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸ Ð¸Ð· ÑÐ¿Ð¸ÑÐºÐ° ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹ Ð²Ñ‹ÑˆÐµ.
ÐšÐ°Ð¶Ð´Ð¾Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ðµ - ÑÑ‚Ñ€Ð¾ÐºÐ°.

Ð’Ñ‹Ð²ÐµÐ´Ð¸ Ð¢ÐžÐ›Ð¬ÐšÐž JSON, Ð±ÐµÐ· ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ²."""

    return prompt


def _build_synthesis_prompt(
    segment_results: List[Dict[str, Any]],
    transcription: str,
    template_variables: Dict[str, str],
    diarization_analysis: Optional[Dict[str, Any]] = None
) -> str:
    """
    ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð° Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð° Ð¸Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²
    """
    # Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²
    segments_summary = ""
    for i, result in enumerate(segment_results):
        segments_summary += f"\n--- Ð¡Ð•Ð“ÐœÐ•ÐÐ¢ {i + 1} ---\n"
        segments_summary += json.dumps(result, ensure_ascii=False, indent=2)
        segments_summary += "\n"
    
    # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð°Ð½Ð°Ð»Ð¸Ð· Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ
    diarization_context = ""
    if diarization_analysis:
        speakers_info = diarization_analysis.get('speakers', {})
        if speakers_info:
            diarization_context = "\n\nÐÐÐÐ›Ð˜Ð— Ð£Ð§ÐÐ¡Ð¢ÐÐ˜ÐšÐžÐ’ Ð’Ð¡Ð¢Ð Ð•Ð§Ð˜:\n"
            for speaker_id, info in speakers_info.items():
                role = info.get('role', 'ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸Ðº')
                time_percent = info.get('speaking_time_percent', 0)
                diarization_context += f"- {speaker_id} ({role}): {time_percent:.1f}% Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸\n"
    
    variables_str = "\n".join([f"- {key}: {desc}" for key, desc in template_variables.items()])
    
    prompt = f"""CHAIN-OF-THOUGHT: Ð¡Ð˜ÐÐ¢Ð•Ð— Ð¤Ð˜ÐÐÐ›Ð¬ÐÐžÐ“Ðž ÐŸÐ ÐžÐ¢ÐžÐšÐžÐ›Ð

Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ÐÐÐÐ›Ð˜Ð—Ð Ð¡Ð•Ð“ÐœÐ•ÐÐ¢ÐžÐ’:
{segments_summary}
{diarization_context}

Ð—ÐÐ”ÐÐ§Ð:
ÐžÐ±ÑŠÐµÐ´Ð¸Ð½Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ð²ÑÐµÑ… ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ð² ÐµÐ´Ð¸Ð½Ñ‹Ð¹ ÑÐ²ÑÐ·Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð» Ð´Ð»Ñ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð¹:
{variables_str}

Ð˜ÐÐ¡Ð¢Ð Ð£ÐšÐ¦Ð˜Ð˜ ÐŸÐž Ð¡Ð˜ÐÐ¢Ð•Ð—Ð£:
1. ÐžÐ‘ÐªÐ•Ð”Ð˜ÐÐ•ÐÐ˜Ð•: Ð¡Ð¾Ð±ÐµÑ€Ð¸ Ð²ÑÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ð² ÐµÐ´Ð¸Ð½Ð¾Ðµ Ñ†ÐµÐ»Ð¾Ðµ
2. Ð”Ð•Ð”Ð£ÐŸÐ›Ð˜ÐšÐÐ¦Ð˜Ð¯: Ð£Ð´Ð°Ð»Ð¸ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€ÑÑŽÑ‰ÑƒÑŽÑÑ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¼ÐµÐ¶Ð´Ñƒ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°Ð¼Ð¸
3. Ð¥Ð ÐžÐÐžÐ›ÐžÐ“Ð˜Ð¯: Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸ Ñ…Ñ€Ð¾Ð½Ð¾Ð»Ð¾Ð³Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð¿Ð¾Ñ€ÑÐ´Ð¾Ðº ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹
4. Ð¡Ð’Ð¯Ð—ÐÐžÐ¡Ð¢Ð¬: Ð¡Ð¾Ð·Ð´Ð°Ð¹ ÑÐ²ÑÐ·Ð½Ð¾Ðµ Ð¿Ð¾Ð²ÐµÑÑ‚Ð²Ð¾Ð²Ð°Ð½Ð¸Ðµ, Ð° Ð½Ðµ ÑÐ¿Ð¸ÑÐ¾Ðº Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ð¾Ð²
5. ÐŸÐžÐ›ÐÐžÐ¢Ð: Ð’ÐºÐ»ÑŽÑ‡Ð¸ Ð²ÑÑŽ Ð²Ð°Ð¶Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²
6. ÐšÐžÐÐ¢Ð•ÐšÐ¡Ð¢: Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¾ ÑÐ¿Ð¸ÐºÐµÑ€Ð°Ñ… Ð´Ð»Ñ ÑƒÑ‚Ð¾Ñ‡Ð½ÐµÐ½Ð¸Ñ Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ…

Ð¡ÐŸÐ•Ð¦Ð˜ÐÐ›Ð¬ÐÐ«Ð• ÐŸÐ ÐÐ’Ð˜Ð›Ð:
- Ð•ÑÐ»Ð¸ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¸Ð· Ñ€Ð°Ð·Ð½Ñ‹Ñ… ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² ÐºÐ¾Ð½Ñ„Ð»Ð¸ÐºÑ‚ÑƒÐµÑ‚ - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð±Ð¾Ð»ÐµÐµ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½ÑƒÑŽ
- ÐžÐ±ÑŠÐµÐ´Ð¸Ð½ÑÐ¹ Ð¿Ð¾Ñ…Ð¾Ð¶Ð¸Ðµ Ð¿ÑƒÐ½ÐºÑ‚Ñ‹ Ð² ÑÐ¿Ð¸ÑÐºÐ°Ñ…
- Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸ Ð¸ Ñ€ÐµÑˆÐµÐ½Ð¸Ñ Ð¿Ð¾ ÑÐ¼Ñ‹ÑÐ»Ð¾Ð²Ñ‹Ð¼ Ð±Ð»Ð¾ÐºÐ°Ð¼
- Ð”Ð»Ñ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð²: Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½Ð¸ Ð²ÑÐµÑ… ÑƒÐ¿Ð¾Ð¼ÑÐ½ÑƒÑ‚Ñ‹Ñ…, ÑƒÐºÐ°Ð¶Ð¸ Ñ€Ð¾Ð»Ð¸ ÐµÑÐ»Ð¸ Ð¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹
- Ð”Ð»Ñ Ð·Ð°Ð´Ð°Ñ‡: ÑƒÐºÐ°Ð¶Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ñ… Ð¸Ð· Ñ‡Ð¸ÑÐ»Ð° ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð² ÐµÑÐ»Ð¸ Ð²Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾

ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž â€” Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹:
- Ð’Ð¡Ð• Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ñ Ð² JSON Ð´Ð¾Ð»Ð¶Ð½Ñ‹ Ð±Ñ‹Ñ‚ÑŒ ÐŸÐ ÐžÐ¡Ð¢Ð«ÐœÐ˜ Ð¡Ð¢Ð ÐžÐšÐÐœÐ˜ (string)
- ÐÐ• Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð²Ð»Ð¾Ð¶ÐµÐ½Ð½Ñ‹Ðµ Ð¾Ð±ÑŠÐµÐºÑ‚Ñ‹ {{}} Ð¸Ð»Ð¸ Ð¼Ð°ÑÑÐ¸Ð²Ñ‹ [] Ð² ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ðµ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸Ð¹
- Ð¡Ð¿Ð¸ÑÐºÐ¸ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐ¹ ÐºÐ°Ðº Ð¼Ð½Ð¾Ð³Ð¾ÑÑ‚Ñ€Ð¾Ñ‡Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚: "- Ð¿ÑƒÐ½ÐºÑ‚1\\n- Ð¿ÑƒÐ½ÐºÑ‚2\\n- Ð¿ÑƒÐ½ÐºÑ‚3"
- Ð”Ð°Ñ‚Ñ‹: Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ñ‚ÐµÐºÑÑ‚ Ñ‚Ð¸Ð¿Ð° "20 Ð¾ÐºÑ‚ÑÐ±Ñ€Ñ 2024", ÐÐ• {{"day": 20}}
- Ð£Ñ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¸: "Ð˜Ð¼Ñ (Ñ€Ð¾Ð»ÑŒ); Ð˜Ð¼Ñ2 (Ñ€Ð¾Ð»ÑŒ2); ...", ÐÐ• [{{"name": "Ð˜Ð¼Ñ"}}]
- Ð’Ñ€ÐµÐ¼Ñ: "14:30" Ð¸Ð»Ð¸ "Ñ 14:00 Ð´Ð¾ 15:30", ÐÐ• {{"start": "14:00"}}

ÐŸÐ Ð˜ÐœÐ•Ð  ÐŸÐ ÐÐ’Ð˜Ð›Ð¬ÐÐžÐ“Ðž Ð’Ð«Ð’ÐžÐ”Ð:
{{
  "date": "20 Ð¾ÐºÑ‚ÑÐ±Ñ€Ñ 2024",
  "participants": "ÐžÐºÑÐ°Ð½Ð°, Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº; Ð“Ð°Ð»Ñ, Ð¼ÐµÑ‚Ð¾Ð´Ð¾Ð»Ð¾Ð³; ÐÐ»ÐµÐºÑÐµÐ¹ Ð¢Ð¸Ð¼Ñ‡ÐµÐ½ÐºÐ¾, Ð¸Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ‚Ð¾Ñ€",
  "decisions": "- Ð‘Ð»Ð¾ÐºÐ¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ Ñ€ÐµÐ´Ð°ÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ñ„Ð°ÐºÑ‚Ð° Ð´Ð»Ñ ÑÑ‚Ñ€Ð¾Ðº Ñ Ð¿Ð¾Ð´Ñ‚Ð²ÐµÑ€Ð¶Ð´ÐµÐ½Ð½Ñ‹Ð¼Ð¸ Ð¼Ð°Ñ€ÐºÐ°Ð¼Ð¸\\n- ÐÐµ ÑÑ‚Ð¾Ð¿Ð¾Ñ€Ð¸Ñ‚ÑŒ Ð¿Ð¾Ñ‚Ð¾Ðº Ð¸Ð·-Ð·Ð° Ð¾ÑˆÐ¸Ð±Ð¾Ðº\\n- ÐžÑ„Ð¾Ñ€Ð¼Ð¸Ñ‚ÑŒ Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ CAP-Ð·Ð°Ð´Ð°Ñ‡Ð¸"
}}

Ð¤ÐžÐ ÐœÐÐ¢ Ð’Ð«Ð’ÐžÐ”Ð:
JSON-Ð¾Ð±ÑŠÐµÐºÑ‚ Ñ Ñ‚ÐµÐ¼Ð¸ Ð¶Ðµ ÐºÐ»ÑŽÑ‡Ð°Ð¼Ð¸, Ð½Ð¾ Ñ Ð¾Ð±ÑŠÐµÐ´Ð¸Ð½ÐµÐ½Ð½Ð¾Ð¹ Ð¸ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÐµÐ¹.
Ð’Ñ‹Ð²ÐµÐ´Ð¸ Ð¢ÐžÐ›Ð¬ÐšÐž JSON, Ð±ÐµÐ· ÐºÐ¾Ð¼Ð¼ÐµÐ½Ñ‚Ð°Ñ€Ð¸ÐµÐ²."""

    return prompt


async def _process_single_segment(
    segment: 'TranscriptionSegment',
    segment_idx: int,
    total_segments: int,
    client: openai.OpenAI,
    selected_model: str,
    system_prompt: str,
    template_variables: Dict[str, str],
    extra_headers: Dict[str, str],
    retry_manager: RetryManager
) -> Tuple[int, Dict[str, Any]]:
    """
    ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ð¾Ð´Ð¸Ð½ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸
    
    Args:
        segment: Ð¡ÐµÐ³Ð¼ÐµÐ½Ñ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸
        segment_idx: Ð˜Ð½Ð´ÐµÐºÑ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°
        total_segments: ÐžÐ±Ñ‰ÐµÐµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²
        client: OpenAI ÐºÐ»Ð¸ÐµÐ½Ñ‚
        selected_model: ÐœÐ¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ
        system_prompt: Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚
        template_variables: ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð°
        extra_headers: Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ HTTP Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ¸
        retry_manager: ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ Ð´Ð»Ñ Ð¿Ð¾Ð²Ñ‚Ð¾Ñ€Ð½Ñ‹Ñ… Ð¿Ð¾Ð¿Ñ‹Ñ‚Ð¾Ðº
        
    Returns:
        ÐšÐ¾Ñ€Ñ‚ÐµÐ¶ (Ð¸Ð½Ð´ÐµÐºÑ_ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°, Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚_Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸)
    """
    logger.info(f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð° {segment_idx + 1}/{total_segments}")
    
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ, Ð¸Ð½Ð°Ñ‡Ðµ Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹
    segment_text = segment.formatted_text if segment.formatted_text else segment.text
    
    # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ Ð´Ð»Ñ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°
    segment_prompt = _build_segment_analysis_prompt(
        segment_text=segment_text,
        segment_id=segment_idx,
        total_segments=total_segments,
        template_variables=template_variables
    )
    
    # Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð²Ñ‹Ð·Ð¾Ð²Ð° OpenAI API
    async def _call_openai_api():
        return await asyncio.to_thread(
            client.chat.completions.create,
            model=selected_model,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": segment_prompt}
            ],
            temperature=0.1,
            response_format={"type": "json_object"},
            extra_headers=extra_headers
        )
    
    # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÑÐµÐ¼ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ñ retry Ð»Ð¾Ð³Ð¸ÐºÐ¾Ð¹
    response = await retry_manager.execute_with_retry(_call_openai_api)
    content = response.choices[0].message.content
    
    # ÐŸÐ°Ñ€ÑÐ¸Ð¼ JSON Ð¾Ñ‚Ð²ÐµÑ‚
    segment_result = json.loads(content)
    
    logger.info(f"Ð¡ÐµÐ³Ð¼ÐµÐ½Ñ‚ {segment_idx + 1} Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ð½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
    
    return (segment_idx, segment_result)


async def generate_protocol_chain_of_thought(
    manager: 'LLMManager',
    provider_name: str,
    transcription: str,
    template_variables: Dict[str, str],
    segments: List['TranscriptionSegment'],
    diarization_data: Optional[Dict[str, Any]] = None,
    diarization_analysis: Optional[Dict[str, Any]] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    Chain-of-Thought Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð° Ð´Ð»Ñ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ñ… Ð²ÑÑ‚Ñ€ÐµÑ‡
    
    Ð­Ñ‚Ð°Ð¿Ñ‹:
    1. ÐÐ½Ð°Ð»Ð¸Ð· ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð° Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ð¾
    2. Ð¡Ð¸Ð½Ñ‚ÐµÐ· Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð° Ð¸Ð· Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð² ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²
    
    Args:
        manager: ÐœÐµÐ½ÐµÐ´Ð¶ÐµÑ€ LLM
        provider_name: ÐÐ°Ð·Ð²Ð°Ð½Ð¸Ðµ Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð°
        transcription: ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ñ‚ÐµÐºÑÑ‚ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸
        template_variables: ÐŸÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ ÑˆÐ°Ð±Ð»Ð¾Ð½Ð°
        segments: Ð¡Ð¿Ð¸ÑÐ¾Ðº ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸Ð¸
        diarization_data: Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        diarization_analysis: ÐÐ½Ð°Ð»Ð¸Ð· Ð´Ð¸Ð°Ñ€Ð¸Ð·Ð°Ñ†Ð¸Ð¸
        **kwargs: Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹
        
    Returns:
        Ð¤Ð¸Ð½Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»
    """
    logger.info(f"ÐÐ°Ñ‡Ð°Ð»Ð¾ Chain-of-Thought Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð´Ð»Ñ {len(segments)} ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
    
    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÐ¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚ (Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ ÐºÐ»Ð°ÑÑÐ¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸)
    system_prompt = _build_system_prompt(transcription, diarization_analysis)
    
    segment_results = []
    
    # Ð­Ð¢ÐÐŸ 1: ÐÐ½Ð°Ð»Ð¸Ð· ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°
    logger.info("Ð­Ñ‚Ð°Ð¿ 1: ÐÐ½Ð°Ð»Ð¸Ð· Ð¾Ñ‚Ð´ÐµÐ»ÑŒÐ½Ñ‹Ñ… ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
    
    if provider_name == "openai":
        provider = manager.providers[provider_name]
        openai_model_key = kwargs.get("openai_model_key")
        
        # Ð’Ñ‹Ð±Ð¾Ñ€ Ð¿Ñ€ÐµÑÐµÑ‚Ð° Ð¼Ð¾Ð´ÐµÐ»Ð¸
        selected_model = settings.openai_model
        selected_base_url = settings.openai_base_url or "https://api.openai.com/v1"
        
        if openai_model_key:
            try:
                preset = next((p for p in settings.openai_models if p.key == openai_model_key), None)
                if preset:
                    selected_model = preset.model
                    if getattr(preset, 'base_url', None):
                        selected_base_url = preset.base_url
            except Exception:
                pass
        
        # ÐšÐ»Ð¸ÐµÐ½Ñ‚ Ð´Ð»Ñ Ð½ÑƒÐ¶Ð½Ð¾Ð³Ð¾ base_url
        client = provider.client
        if client is None or (selected_base_url and getattr(client, 'base_url', None) != selected_base_url):
            client = openai.OpenAI(
                api_key=settings.openai_api_key,
                base_url=selected_base_url,
                http_client=provider.http_client
            )
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ extra_headers Ð´Ð»Ñ Ð°Ñ‚Ñ€Ð¸Ð±ÑƒÑ†Ð¸Ð¸
        extra_headers = {}
        if settings.http_referer:
            extra_headers["HTTP-Referer"] = settings.http_referer
        if settings.x_title:
            extra_headers["X-Title"] = settings.x_title
        
        # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ retry manager Ð´Ð»Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²
        retry_manager = RetryManager(LLM_RETRY_CONFIG)
        
        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ Ð½Ð°ÑÑ‚Ñ€Ð¾Ð¹ÐºÑƒ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»Ð¸Ð·Ð¼Ð°
        max_parallel = settings.max_parallel_segments
        
        # ÐžÐ±Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÐ¼ ÐºÐ°Ð¶Ð´Ñ‹Ð¹ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚ Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾
        if max_parallel:
            logger.info(
                f"Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ {len(segments)} ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² "
                f"(Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ: {max_parallel} Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾)"
            )
            semaphore = asyncio.Semaphore(max_parallel)
            
            async def _process_with_semaphore(segment):
                async with semaphore:
                    return await _process_single_segment(
                        segment=segment,
                        segment_idx=segment.segment_id,
                        total_segments=len(segments),
                        client=client,
                        selected_model=selected_model,
                        system_prompt=system_prompt,
                        template_variables=template_variables,
                        extra_headers=extra_headers,
                        retry_manager=retry_manager
                    )
            
            tasks = [_process_with_semaphore(segment) for segment in segments]
        else:
            logger.info(f"Ð—Ð°Ð¿ÑƒÑÐº Ð¿Ð°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð¾Ð¹ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ {len(segments)} ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² (Ð±ÐµÐ· Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ð¹)")
            
            tasks = [
                _process_single_segment(
                    segment=segment,
                    segment_idx=segment.segment_id,
                    total_segments=len(segments),
                    client=client,
                    selected_model=selected_model,
                    system_prompt=system_prompt,
                    template_variables=template_variables,
                    extra_headers=extra_headers,
                    retry_manager=retry_manager
                )
                for segment in segments
            ]
        
        # ÐŸÐ°Ñ€Ð°Ð»Ð»ÐµÐ»ÑŒÐ½Ð°Ñ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²ÑÐµÑ… ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²
        successful_count = 0
        failed_count = 0
        
        # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ Ð¸Ð½Ð´ÐµÐºÑÑƒ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð° Ð´Ð»Ñ ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ñ Ð¿Ð¾Ñ€ÑÐ´ÐºÐ°
        for result in results:
            if isinstance(result, Exception):
                failed_count += 1
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐµ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°: {result}")
                # Ð”Ð¾Ð±Ð°Ð²Ð»ÑÐµÐ¼ Ð¿ÑƒÑÑ‚Ð¾Ð¹ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚
                segment_results.append({
                    key: "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ¸ ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð°" 
                    for key in template_variables.keys()
                })
            else:
                successful_count += 1
                segment_id, data = result
                segment_results.append(data)
        
        logger.info(
            f"ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð² Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð°: ÑƒÑÐ¿ÐµÑˆÐ½Ð¾ {successful_count}/{len(segments)}, "
            f"Ð¾ÑˆÐ¸Ð±Ð¾Ðº {failed_count}/{len(segments)}"
        )
        
        # Ð­Ð¢ÐÐŸ 2: Ð¡Ð¸Ð½Ñ‚ÐµÐ· Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð°
        logger.info("Ð­Ñ‚Ð°Ð¿ 2: Ð¡Ð¸Ð½Ñ‚ÐµÐ· Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ð¿Ñ€Ð¾Ñ‚Ð¾ÐºÐ¾Ð»Ð° Ð¸Ð· ÑÐµÐ³Ð¼ÐµÐ½Ñ‚Ð¾Ð²")
        
        synthesis_prompt = _build_synthesis_prompt(
            segment_results=segment_results,
            transcription=transcription,
            template_variables=template_variables,
            diarization_analysis=diarization_analysis
        )
        
        async def _call_openai_synthesis():
            return await asyncio.to_thread(
                client.chat.completions.create,
                model=selected_model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": synthesis_prompt}
                ],
                temperature=0.1,
                response_format={"type": "json_object"},
                extra_headers=extra_headers
            )
        
        response_synthesis = await _call_openai_synthesis()
        content_synthesis = response_synthesis.choices[0].message.content
        
        try:
            final_protocol = json.loads(content_synthesis)
        except json.JSONDecodeError as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° JSON Ð½Ð° ÑÑ‚Ð°Ð¿Ðµ ÑÐ¸Ð½Ñ‚ÐµÐ·Ð°: {e}")
            start_idx = content_synthesis.find('{')
            end_idx = content_synthesis.rfind('}') + 1
            json_str = content_synthesis[start_idx:end_idx] if start_idx != -1 and end_idx > start_idx else content_synthesis
            final_protocol = json.loads(json_str)
        
        logger.info("Chain-of-Thought Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð° ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
        return final_protocol
    
    else:
        # Ð”Ð»Ñ Ð´Ñ€ÑƒÐ³Ð¸Ñ… Ð¿Ñ€Ð¾Ð²Ð°Ð¹Ð´ÐµÑ€Ð¾Ð² Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´
        logger.warning(
            f"Chain-of-Thought Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ÑÑ Ð´Ð»Ñ {provider_name}, "
            f"Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ Ð¿Ð¾Ð´Ñ…Ð¾Ð´"
        )
        return await manager.generate_protocol(
            provider_name, transcription, template_variables, diarization_data, **kwargs
        )


# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ Ð¼ÐµÐ½ÐµÐ´Ð¶ÐµÑ€Ð° LLM
llm_manager = LLMManager()
